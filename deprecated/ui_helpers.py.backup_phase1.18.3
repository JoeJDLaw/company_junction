"""
UI helper functions for Phase 1.17.1.

This module provides pure functions for run loading, stage status parsing,
and artifact path management for the Streamlit UI.
"""

import json
import os
import time
import hashlib
import yaml
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pyarrow as pa  # type: ignore
import pyarrow.parquet as pq  # type: ignore
import pyarrow.compute as pc  # type: ignore
import pyarrow.dataset as ds  # type: ignore

# Optional DuckDB import for fallback
try:
    import duckdb

    DUCKDB_AVAILABLE = True
except ImportError:
    DUCKDB_AVAILABLE = False

from src.utils.cache_utils import get_latest_run_id, load_run_index
from src.utils.logging_utils import get_logger

logger = get_logger(__name__)


class PageFetchTimeout(Exception):
    """Exception raised when page fetch exceeds timeout."""

    pass


def _is_non_empty(obj: Any) -> bool:
    """
    Safely check if an object is non-empty, handling pandas, numpy, and pyarrow objects.

    Args:
        obj: Object to check

    Returns:
        True if object is non-empty, False otherwise
    """
    if obj is None:
        return False
    try:
        import pandas as pd

        if isinstance(obj, (pd.Series, pd.DataFrame)):
            return not obj.empty
    except ImportError:
        pass
    except Exception:
        pass
    try:
        import numpy as np

        if isinstance(obj, np.ndarray):
            return obj.size > 0
    except ImportError:
        pass
    except Exception:
        pass
    try:
        import pyarrow as pa

        if isinstance(obj, pa.Table):
            return obj.num_rows > 0
        if isinstance(obj, (pa.Array, pa.ChunkedArray)):
            return len(obj) > 0
    except ImportError:
        pass
    except Exception:
        pass
    if hasattr(obj, "__len__"):
        try:
            return len(obj) > 0
        except Exception:
            return False
    return True


def list_runs() -> List[Dict[str, Any]]:
    """
    Get a sorted list of all runs with metadata.

    Returns:
        List of run dictionaries sorted by timestamp (newest first)
    """
    run_index = load_run_index()
    runs = []

    for run_id, run_data in run_index.items():
        runs.append(
            {
                "run_id": run_id,
                "timestamp": run_data.get("timestamp", ""),
                "status": run_data.get("status", "unknown"),
                "input_paths": run_data.get("input_paths", []),
                "config_paths": run_data.get("config_paths", []),
                "input_hash": run_data.get("input_hash", ""),
                "config_hash": run_data.get("config_hash", ""),
                "dag_version": run_data.get("dag_version", "1.0.0"),
            }
        )

    # Sort by timestamp (newest first)
    runs.sort(key=lambda x: x["timestamp"], reverse=True)

    return runs


def get_run_metadata(run_id: str) -> Optional[Dict[str, Any]]:
    """
    Get detailed metadata for a specific run.

    Args:
        run_id: The run ID to get metadata for

    Returns:
        Run metadata dictionary or None if run not found
    """
    import logging

    logger = logging.getLogger(__name__)

    run_index = load_run_index()

    if run_id not in run_index:
        logger.warning(f"Run {run_id} not found in run_index")
        return None

    run_data = run_index[run_id]

    # Parse timestamp for display
    try:
        timestamp = datetime.fromisoformat(run_data.get("timestamp", ""))
        formatted_timestamp = timestamp.strftime("%Y-%m-%d %H:%M:%S")
    except (ValueError, TypeError) as e:
        logger.warning(f"Failed to parse timestamp: {e}")
        formatted_timestamp = run_data.get("timestamp", "Unknown")

    result = {
        "run_id": run_id,
        "timestamp": run_data.get("timestamp", ""),
        "formatted_timestamp": formatted_timestamp,
        "status": run_data.get("status", "unknown"),
        "input_paths": run_data.get("input_paths", []),
        "config_paths": run_data.get("config_paths", []),
        "input_hash": run_data.get("input_hash", ""),
        "config_hash": run_data.get("config_hash", ""),
        "dag_version": run_data.get("dag_version", "1.0.0"),
    }

    return result


def validate_run_artifacts(run_id: str) -> Dict[str, Any]:
    """
    Validate that a run has all required artifacts.

    Args:
        run_id: The run ID to validate

    Returns:
        Dictionary with validation results
    """
    validation: Dict[str, Any] = {
        "run_exists": False,
        "status": "unknown",
        "has_review_ready_csv": False,
        "has_review_ready_parquet": False,
        "has_pipeline_state": False,
        "has_review_meta": False,
        "missing_files": [],
        "errors": [],
    }

    # Check if run exists in index
    run_metadata = get_run_metadata(run_id)
    if not run_metadata:
        validation["errors"].append(f"Run {run_id} not found in run index")
        return validation

    validation["run_exists"] = True
    validation["status"] = run_metadata["status"]

    # Check required files
    processed_dir = f"data/processed/{run_id}"
    interim_dir = f"data/interim/{run_id}"

    # Check review_ready files
    csv_path = f"{processed_dir}/review_ready.csv"
    parquet_path = f"{processed_dir}/review_ready.parquet"

    if os.path.exists(csv_path):
        validation["has_review_ready_csv"] = True
    else:
        validation["missing_files"].append("review_ready.csv")

    if os.path.exists(parquet_path):
        validation["has_review_ready_parquet"] = True
    else:
        validation["missing_files"].append("review_ready.parquet")

    # Check pipeline state
    state_path = f"{interim_dir}/pipeline_state.json"
    if os.path.exists(state_path):
        validation["has_pipeline_state"] = True
    else:
        validation["missing_files"].append("pipeline_state.json")

    # Check review meta
    meta_path = f"{processed_dir}/review_meta.json"
    if os.path.exists(meta_path):
        validation["has_review_meta"] = True
    else:
        validation["missing_files"].append("review_meta.json")

    return validation


def load_stage_state(run_id: str) -> Optional[Dict[str, Any]]:
    """
    Load and parse MiniDAG stage state for a run.

    Args:
        run_id: The run ID to load state for

    Returns:
        Parsed stage state or None if not available
    """
    state_path = f"data/interim/{run_id}/pipeline_state.json"

    if not os.path.exists(state_path):
        return None

    try:
        with open(state_path, "r") as f:
            state_data = json.load(f)

        if not isinstance(state_data, dict):
            return None

        stages = state_data.get("stages", {})
        metadata = state_data.get("metadata", {})

        # Parse stage information
        stage_info = []
        for stage_name, stage_data in stages.items():
            if isinstance(stage_data, dict):
                start_time = stage_data.get("start_time")
                end_time = stage_data.get("end_time")

                # Calculate duration
                duration = 0.0
                if (
                    start_time is not None
                    and end_time is not None
                    and start_time > 0
                    and end_time > 0
                ):
                    duration = end_time - start_time

                # Format timestamps
                start_str = ""
                end_str = ""
                if start_time is not None and start_time > 0:
                    start_str = datetime.fromtimestamp(start_time).strftime("%H:%M:%S")
                if end_time is not None and end_time > 0:
                    end_str = datetime.fromtimestamp(end_time).strftime("%H:%M:%S")

                stage_info.append(
                    {
                        "name": stage_name,
                        "status": stage_data.get("status", "unknown"),
                        "start_time": start_time,
                        "end_time": end_time,
                        "start_str": start_str,
                        "end_str": end_str,
                        "duration": duration,
                        "duration_str": f"{duration:.2f}s" if duration > 0 else "N/A",
                    }
                )

        return {
            "stages": stage_info,
            "metadata": metadata,
            "run_id": run_id,
        }

    except (json.JSONDecodeError, IOError) as e:
        logger.warning(f"Failed to load stage state for {run_id}: {e}")
        return None


def get_artifact_paths(run_id: str) -> Dict[str, str]:
    """
    Get paths to artifacts for a run.

    Args:
        run_id: The run ID to get paths for

    Returns:
        Dictionary mapping artifact names to file paths
    """
    processed_dir = f"data/processed/{run_id}"
    interim_dir = f"data/interim/{run_id}"

    return {
        "review_ready_csv": f"{processed_dir}/review_ready.csv",
        "review_ready_parquet": f"{processed_dir}/review_ready.parquet",
        "review_meta": f"{processed_dir}/review_meta.json",
        "pipeline_state": f"{interim_dir}/pipeline_state.json",
        "candidate_pairs": f"{interim_dir}/candidate_pairs.parquet",
        "groups": f"{interim_dir}/groups.parquet",
        "survivorship": f"{interim_dir}/survivorship.parquet",
        "dispositions": f"{interim_dir}/dispositions.parquet",
        "alias_matches": f"{interim_dir}/alias_matches.parquet",
        "block_top_tokens": f"{interim_dir}/block_top_tokens.csv",
    }


def get_default_run_id() -> str:
    """
    Get the default run ID (latest successful run).

    Returns:
        Latest run ID or empty string if none available
    """
    latest_run_id = get_latest_run_id()
    if latest_run_id:
        return latest_run_id

    # Fallback: get the most recent complete run
    runs = list_runs()
    for run in runs:
        if run["status"] == "complete":
            return str(run["run_id"])

    return ""


def format_run_display_name(
    run_id: str, metadata: Optional[Dict[str, Any]] = None
) -> str:
    """
    Format a run ID for display in the UI.

    Args:
        run_id: The run ID
        metadata: Optional run metadata

    Returns:
        Formatted display name
    """

    if not metadata:
        metadata = get_run_metadata(run_id)

    if not metadata:
        return run_id

    # Extract input file name
    input_paths = metadata.get("input_paths", [])
    input_file = "Unknown"
    if input_paths:
        input_file = str(Path(input_paths[0]).name)

    # Format timestamp
    timestamp = metadata.get("formatted_timestamp", "Unknown")

    result = f"{input_file} ({timestamp})"
    return result


def get_run_status_icon(status: str) -> str:
    """
    Get status icon for display.

    Args:
        status: Run status string

    Returns:
        Status icon string
    """
    status_icons = {
        "complete": "✅",
        "running": "⏳",
        "failed": "❌",
        "interrupted": "⚠️",
        "unknown": "❓",
    }

    return str(status_icons.get(status, "❓"))


def get_stage_status_icon(status: str) -> str:
    """
    Get stage status icon for display.

    Args:
        status: Stage status string

    Returns:
        Status icon string
    """
    status_icons = {
        "completed": "✅",
        "running": "⏳",
        "failed": "❌",
        "interrupted": "⚠️",
        "pending": "⏸️",
        "unknown": "❓",
    }

    return str(status_icons.get(status, "❓"))


def build_sort_expression(sort_key: str) -> List[Tuple[str, str]]:
    """
    Build PyArrow sort keys for stable sorting.

    Args:
        sort_key: Sort key from dropdown (e.g., "Group Size (Desc)")

    Returns:
        List of (field, direction) tuples for sorting
    """
    # Extract sort field and direction
    if "Group Size" in sort_key:
        field = "group_size"
    elif "Max Score" in sort_key:
        field = "max_score"
    elif "Account Name" in sort_key:
        field = "primary_name"
    else:
        # Default to group_id for stability
        field = "group_id"

    # Determine sort direction
    direction = (
        "ascending" if "(Asc)" in sort_key or "(Desc)" not in sort_key else "descending"
    )

    # Return sort keys with group_id tiebreaker for stability
    return [(field, direction), ("group_id", "ascending")]


def get_groups_page_pyarrow(
    run_id: str, sort_key: str, page: int, page_size: int, filters: Dict[str, Any]
) -> Tuple[List[Dict[str, Any]], int]:
    """
    Get a page of groups using PyArrow for server-side pagination.

    Args:
        run_id: Run ID to load data from
        sort_key: Sort key from dropdown
        page: Page number (1-based)
        page_size: Number of groups per page
        filters: Dictionary of active filters

    Returns:
        Tuple of (groups_data, total_count)
    """
    start_time = time.time()
    step_start = time.time()

    try:
        # Get artifact paths
        artifact_paths = get_artifact_paths(run_id)
        parquet_path = artifact_paths["review_ready_parquet"]

        if not os.path.exists(parquet_path):
            logger.warning(f"Parquet file not found: {parquet_path}")
            return [], 0

        # Log start
        logger.info(
            f'Groups page fetch start | run_id={run_id} sort="{sort_key}" page={page} page_size={page_size}'
        )

        # Check timeout periodically
        def check_timeout():
            elapsed = time.time() - start_time
            if elapsed > 30:  # 30 second timeout
                raise PageFetchTimeout(
                    f"Page fetch exceeded 30 second timeout after {elapsed:.1f}s"
                )

        # Load settings for timeout (hardcoded to 30 seconds for now)
        pass  # type: ignore

        # Step 1: Dataset creation
        step_start = time.time()
        dataset = ds.dataset(parquet_path)
        dataset_time = time.time() - step_start
        logger.info(f"Dataset creation | run_id={run_id} elapsed={dataset_time:.3f}s")

        check_timeout()

        # Step 2: Column projection
        step_start = time.time()
        schema = dataset.schema
        available_columns = schema.names

        header_columns = [
            "group_id",
            "account_name",
            "is_primary",
            "weakest_edge_to_primary",
            "Disposition",
        ]
        # Only include columns that exist
        existing_columns = [col for col in header_columns if col in available_columns]

        logger.info(
            f"Column projection | run_id={run_id} available={len(available_columns)} projected={len(existing_columns)} columns={existing_columns}"
        )

        # Use scanner for projection
        scanner = dataset.scanner(columns=existing_columns)
        projected_table = scanner.to_table()
        projection_time = time.time() - step_start

        logger.info(
            f"Projection complete | run_id={run_id} rows={projected_table.num_rows} elapsed={projection_time:.3f}s"
        )

        check_timeout()

        # Step 3: Apply filters
        step_start = time.time()
        filtered_table = apply_filters_pyarrow(projected_table, filters)
        filter_time = time.time() - step_start

        logger.info(
            f"Filters applied | run_id={run_id} before={projected_table.num_rows} after={filtered_table.num_rows} elapsed={filter_time:.3f}s"
        )

        check_timeout()

        # Step 4: Compute group statistics
        step_start = time.time()
        groups_table = compute_group_stats_pyarrow(filtered_table)
        stats_time = time.time() - step_start

        logger.info(
            f"Group stats computed | run_id={run_id} groups={groups_table.num_rows} elapsed={stats_time:.3f}s"
        )

        # Auto-switch to DuckDB if group stats take too long
        try:
            from src.utils.config_utils import load_settings

            settings = load_settings()
            max_pyarrow_seconds = settings.get("ui", {}).get(
                "max_pyarrow_group_stats_seconds", 5
            )
        except Exception:
            max_pyarrow_seconds = 5

        if stats_time > max_pyarrow_seconds and DUCKDB_AVAILABLE:
            logger.info(
                f"Auto-switching groups backend to DuckDB | run_id={run_id} reason=pyarrow_groupby_slow elapsed={stats_time:.3f}s"
            )
            # Close current connection and switch to DuckDB
            return get_groups_page_duckdb(run_id, sort_key, page, page_size, filters)

        check_timeout()

        # Get total count
        total_groups = groups_table.num_rows

        if total_groups == 0:
            elapsed = time.time() - start_time
            logger.info(
                f'Groups page loaded | run_id={run_id} rows=0 offset=0 sort="{sort_key}" elapsed={elapsed:.3f}'
            )
            return [], 0

        # Calculate pagination
        offset = (page - 1) * page_size
        limit = page_size

        # Step 5: Calculate pagination
        offset = (page - 1) * page_size
        limit = page_size

        # Step 6: Apply sorting and slicing
        if offset >= total_groups:
            page_data = []
        else:
            step_start = time.time()
            sort_keys = build_sort_expression(sort_key)

            # Sort the groups table
            sorted_table = groups_table.sort_by(sort_keys)
            sort_time = time.time() - step_start

            logger.info(
                f"Sorting applied | run_id={run_id} sort_keys={sort_keys} elapsed={sort_time:.3f}s"
            )

            check_timeout()

            # Step 7: Apply slice (LIMIT/OFFSET before pandas conversion)
            step_start = time.time()
            page_table = sorted_table.slice(offset, limit)
            slice_time = time.time() - step_start

            logger.info(
                f"Slice applied | run_id={run_id} offset={offset} limit={limit} slice_rows={page_table.num_rows} elapsed={slice_time:.3f}s"
            )

            # Step 8: Convert slice to pandas
            step_start = time.time()
            page_data = page_table.to_pylist()
            pandas_time = time.time() - step_start

            logger.info(
                f"Pandas conversion | run_id={run_id} rows={len(page_data)} elapsed={pandas_time:.3f}s"
            )

        elapsed = time.time() - start_time
        logger.info(
            f'Groups page loaded | run_id={run_id} rows={len(page_data)} offset={offset} sort="{sort_key}" elapsed={elapsed:.3f} projected_cols={existing_columns}'
        )

        return page_data, total_groups

    except PageFetchTimeout:
        elapsed = time.time() - start_time
        logger.error(
            f"Groups page load timeout | run_id={run_id} elapsed={elapsed:.3f}"
        )
        raise
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            f'Groups page load failed | run_id={run_id} error="{str(e)}" elapsed={elapsed:.3f}'
        )
        return [], 0


def get_groups_page(
    run_id: str, sort_key: str, page: int, page_size: int, filters: Dict[str, Any]
) -> Tuple[List[Dict[str, Any]], int]:
    """
    Get a page of groups using the configured backend (PyArrow or DuckDB).

    Args:
        run_id: The run ID
        sort_key: The sort key
        page: The page number
        page_size: The page size
        filters: The filters dictionary

    Returns:
        Tuple of (page_data, total_groups)
    """
    # Load settings
    with open("config/settings.yaml", "r") as f:
        settings = yaml.safe_load(f)
    use_duckdb = settings.get("ui", {}).get("use_duckdb_for_groups", False)

    # DuckDB-first routing: when flag is true, route to DuckDB before any PyArrow work
    if use_duckdb and DUCKDB_AVAILABLE:
        logger.info(
            f"Using DuckDB backend for groups | run_id={run_id} reason=flag_true"
        )

        # Persist backend choice in session state (using namespaced keys)
        import streamlit as st

        if "cj" not in st.session_state:
            st.session_state["cj"] = {}
        if "backend" not in st.session_state["cj"]:
            st.session_state["cj"]["backend"] = {}
        if "groups" not in st.session_state["cj"]["backend"]:
            st.session_state["cj"]["backend"]["groups"] = {}

        st.session_state["cj"]["backend"]["groups"][run_id] = "duckdb"

        return get_groups_page_duckdb(run_id, sort_key, page, page_size, filters)
    else:
        logger.info(f"Using PyArrow backend for groups | run_id={run_id}")

        # Persist backend choice in session state (using namespaced keys)
        import streamlit as st

        if "cj" not in st.session_state:
            st.session_state["cj"] = {}
        if "backend" not in st.session_state["cj"]:
            st.session_state["cj"]["backend"] = {}
        if "groups" not in st.session_state["cj"]["backend"]:
            st.session_state["cj"]["backend"]["groups"] = {}

        st.session_state["cj"]["backend"]["groups"][run_id] = "pyarrow"

        return get_groups_page_pyarrow(run_id, sort_key, page, page_size, filters)


def apply_filters_pyarrow(table: pa.Table, filters: Dict[str, Any]) -> pa.Table:
    """
    Apply filters to PyArrow table.

    Args:
        table: PyArrow table to filter
        filters: Dictionary of filters to apply

    Returns:
        Filtered PyArrow table
    """
    filtered_table = table

    # Apply disposition filter
    if filters.get("dispositions") and "Disposition" in table.column_names:
        dispositions = filters["dispositions"]
        mask = pc.is_in(pc.field("Disposition"), pa.array(dispositions))
        filtered_table = filtered_table.filter(mask)

    # Apply edge strength filter
    if filters.get("min_edge_strength", 0.0) > 0.0:
        if "weakest_edge_to_primary" in table.column_names:
            mask = pc.field("weakest_edge_to_primary") >= filters["min_edge_strength"]
            filtered_table = filtered_table.filter(mask)

    # Apply alias filter
    if filters.get("has_aliases", False):
        # Check for records with aliases
        if "alias_cross_refs" in table.column_names:
            mask = pc.string_length(pc.field("alias_cross_refs")) > 2  # More than "[]"
        elif "alias_candidates" in table.column_names:
            mask = pc.string_length(pc.field("alias_candidates")) > 2
        else:
            mask = pc.scalar(False)
        filtered_table = filtered_table.filter(mask)

    return filtered_table


def compute_group_stats_pyarrow(table: pa.Table) -> pa.Table:
    """
    Compute group statistics for sorting.

    Args:
        table: PyArrow table with group data

    Returns:
        PyArrow table with group statistics
    """
    # Convert to pandas for easier group operations
    df = table.to_pandas()

    # Compute group statistics
    stats_data = []
    for group_id in df["group_id"].unique():
        group_data = df[df["group_id"] == group_id]

        # Get group size
        group_size = len(group_data)

        # Get max score
        max_score = (
            group_data["weakest_edge_to_primary"].max()
            if "weakest_edge_to_primary" in group_data.columns
            else 0.0
        )

        # Get primary record's account name
        primary_record = (
            group_data[group_data["is_primary"]].iloc[0]
            if group_data["is_primary"].any()
            else group_data.iloc[0]
        )
        primary_name = primary_record.get("account_name", "")

        stats_data.append(
            {
                "group_id": group_id,
                "group_size": group_size,
                "max_score": max_score,
                "primary_name": primary_name or "",
            }
        )

    return pa.Table.from_pylist(stats_data)


def get_group_details_duckdb(run_id: str, group_id: str) -> List[Dict[str, Any]]:
    """
    Get group details using DuckDB for efficient per-group querying.

    Args:
        run_id: The run ID
        group_id: The specific group ID to fetch details for

    Returns:
        List of dictionaries containing group details
    """
    start_time = time.time()
    step_start = time.time()

    try:
        # Get artifact paths and settings
        artifact_paths = get_artifact_paths(run_id)
        parquet_path = artifact_paths["review_ready_parquet"]

        # Load settings
        with open("config/settings.yaml", "r") as f:
            settings = yaml.safe_load(f)
        duckdb_threads = settings.get("ui", {}).get("duckdb_threads", 4)

        # Projected columns for details UI
        projected_cols = [
            "account_name",
            "account_id",
            "Disposition",
            "is_primary",
            "weakest_edge_to_primary",
            "suffix",
        ]

        logger.info(
            f"Group details fetch start | run_id={run_id} group_id={group_id} projected_cols={projected_cols}"
        )

        # Step 1: DuckDB connection
        conn = duckdb.connect(":memory:")
        conn.execute(f"PRAGMA threads = {duckdb_threads}")
        connect_time = time.time() - step_start
        logger.info(
            f"DuckDB connection (details) | run_id={run_id} elapsed={connect_time:.3f}"
        )

        # Step 2: Build and execute query
        step_start = time.time()
        sql = f"""
        SELECT {', '.join(projected_cols)}
        FROM read_parquet('{parquet_path}')
        WHERE group_id = '{group_id}'
        ORDER BY account_name ASC
        """

        result = conn.execute(sql)
        query_time = time.time() - step_start
        logger.info(
            f"Details query executed | run_id={run_id} group_id={group_id} rows={result.rowcount} elapsed={query_time:.3f}"
        )

        # Step 3: Convert to pandas
        step_start = time.time()
        df = result.df()
        pandas_time = time.time() - step_start
        logger.info(
            f"Details pandas conversion | run_id={run_id} group_id={group_id} rows={len(df)} elapsed={pandas_time:.3f}"
        )

        details_data = df.to_dict("records")
        conn.close()

        elapsed = time.time() - start_time
        logger.info(
            f"Group details loaded | run_id={run_id} group_id={group_id} rows={len(details_data)} elapsed={elapsed:.3f}"
        )

        return details_data

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            f'Group details load failed | run_id={run_id} group_id={group_id} error="{str(e)}" elapsed={elapsed:.3f}'
        )
        raise


def get_group_details_pyarrow(run_id: str, group_id: str) -> List[Dict[str, Any]]:
    """
    Get group details using PyArrow (fallback implementation).

    Args:
        run_id: The run ID
        group_id: The group ID

    Returns:
        List of dictionaries containing group details
    """
    start_time = time.time()
    try:
        # Get artifact paths
        artifact_paths = get_artifact_paths(run_id)
        parquet_path = artifact_paths["review_ready_parquet"]

        if not os.path.exists(parquet_path):
            return []

        # Read parquet file and filter for group
        table = pq.read_table(parquet_path)
        group_mask = pc.equal(pc.field("group_id"), pc.scalar(group_id))
        group_table = table.filter(group_mask)

        if group_table.num_rows == 0:
            return []

        # Convert to pandas for easier processing
        group_df = group_table.to_pandas()

        # Project only the columns shown in details UI
        projected_cols = [
            "account_name",
            "account_id",
            "Disposition",
            "is_primary",
            "weakest_edge_to_primary",
            "suffix",
        ]
        available_cols = [col for col in projected_cols if col in group_df.columns]
        group_df = group_df[available_cols]

        elapsed = time.time() - start_time
        logger.info(
            f"Group details loaded (PyArrow) | run_id={run_id} group_id={group_id} elapsed={elapsed:.3f}"
        )

        return group_df.to_dict("records")

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            f"Failed to load group details (PyArrow): {e} | run_id={run_id} group_id={group_id} elapsed={elapsed:.3f}"
        )
        return []


def get_group_details_lazy(run_id: str, group_id: str) -> List[Dict[str, Any]]:
    """
    Get group details using the configured backend (DuckDB preferred for performance).

    Args:
        run_id: The run ID
        group_id: The group ID

    Returns:
        List of dictionaries containing group details
    """
    # Load settings
    with open("config/settings.yaml", "r") as f:
        settings = yaml.safe_load(f)
    use_duckdb = settings.get("ui", {}).get("use_duckdb_for_groups", False)

    if use_duckdb and DUCKDB_AVAILABLE:
        return get_group_details_duckdb(run_id, group_id)
    else:
        # Fallback to existing PyArrow implementation
        return get_group_details_pyarrow(run_id, group_id)


def build_cache_key(
    run_id: str,
    sort_key: str,
    page: int,
    page_size: int,
    filters: Dict[str, Any],
    backend: str = "pyarrow",
) -> str:
    """
    Build a cache key for groups page data.

    Args:
        run_id: The run ID
        sort_key: The sort key
        page: The page number
        page_size: The page size
        filters: The filters dictionary
        backend: The backend used ("pyarrow" or "duckdb")

    Returns:
        A string cache key
    """
    # Get parquet fingerprint
    try:
        artifact_paths = get_artifact_paths(run_id)
        parquet_path = artifact_paths["review_ready_parquet"]
        stat = os.stat(parquet_path)
        parquet_fingerprint = f"{int(stat.st_mtime)}_{stat.st_size}"
    except Exception:
        parquet_fingerprint = "unknown"

    # Create filters signature
    filters_signature = hashlib.md5(str(sorted(filters.items())).encode()).hexdigest()[
        :8
    ]

    # Build cache key components
    key_components = [
        run_id,
        parquet_fingerprint,
        sort_key,
        str(page),
        str(page_size),
        filters_signature,
        backend,
    ]

    cache_key = hashlib.md5("|".join(key_components).encode()).hexdigest()
    logger.info(
        f'Cache key generated | run_id={run_id} key={cache_key[:8]}... fingerprint={parquet_fingerprint} backend={backend} page={page} size={page_size} sort="{sort_key}"'
    )

    return cache_key


def build_details_cache_key(run_id: str, group_id: str, backend: str = "duckdb") -> str:
    """
    Build a cache key for group details data.

    Args:
        run_id: The run ID
        group_id: The group ID
        backend: The backend used ("pyarrow" or "duckdb")

    Returns:
        A string cache key
    """
    # Get parquet fingerprint
    try:
        artifact_paths = get_artifact_paths(run_id)
        parquet_path = artifact_paths["review_ready_parquet"]
        stat = os.stat(parquet_path)
        parquet_fingerprint = f"{int(stat.st_mtime)}_{stat.st_size}"
    except Exception:
        parquet_fingerprint = "unknown"

    # Build cache key components
    key_components = [run_id, group_id, parquet_fingerprint, backend]

    cache_key = hashlib.md5("|".join(key_components).encode()).hexdigest()
    return cache_key


def get_groups_page_duckdb(
    run_id: str, sort_key: str, page: int, page_size: int, filters: Dict[str, Any]
) -> Tuple[List[Dict[str, Any]], int]:
    """
    Get a page of groups using DuckDB for server-side pagination.

    Args:
        run_id: Run ID to load data from
        sort_key: Sort key from dropdown
        page: Page number (1-based)
        page_size: Number of groups per page
        filters: Dictionary of active filters

    Returns:
        Tuple of (groups_data, total_count)
    """
    start_time = time.time()
    step_start = time.time()

    try:
        # Get artifact paths
        artifact_paths = get_artifact_paths(run_id)
        parquet_path = artifact_paths["review_ready_parquet"]

        if not os.path.exists(parquet_path):
            logger.warning(f"Parquet file not found: {parquet_path}")
            return [], 0

        # Check timeout periodically
        def check_timeout():
            elapsed = time.time() - start_time
            if elapsed > 30:  # 30 second timeout
                raise PageFetchTimeout(
                    f"Page fetch exceeded 30 second timeout after {elapsed:.1f}s"
                )

        # Load settings
        try:
            from src.utils.config_utils import load_settings

            settings = load_settings()
            duckdb_threads = settings.get("ui", {}).get("duckdb_threads", 4)
        except Exception:
            duckdb_threads = 4

        # Log start
        logger.info(
            f'DuckDB groups page fetch start | run_id={run_id} sort="{sort_key}" page={page} page_size={page_size}'
        )

        # Step 1: DuckDB connection
        step_start = time.time()
        conn = duckdb.connect(":memory:")
        conn.execute(f"PRAGMA threads = {duckdb_threads}")
        connect_time = time.time() - step_start

        logger.info(
            f"DuckDB connection | run_id={run_id} threads={duckdb_threads} elapsed={connect_time:.3f}s"
        )

        check_timeout()

        # Step 2: Build SQL query
        step_start = time.time()

        # Build WHERE clause for filters
        where_conditions = []
        if filters.get("dispositions"):
            dispositions = filters["dispositions"]
            disp_list = "', '".join(dispositions)
            where_conditions.append(f"Disposition IN ('{disp_list}')")

        if filters.get("min_edge_strength", 0.0) > 0.0:
            where_conditions.append(
                f"weakest_edge_to_primary >= {filters['min_edge_strength']}"
            )

        where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"

        # Build ORDER BY clause
        order_by_clause = ""
        if "Group Size" in sort_key:
            if "(Desc)" in sort_key:
                order_by_clause = "s.group_size DESC"
            else:
                order_by_clause = "s.group_size ASC"
        elif "Max Score" in sort_key:
            if "(Desc)" in sort_key:
                order_by_clause = "s.max_score DESC"
            else:
                order_by_clause = "s.max_score ASC"
        elif "Account Name" in sort_key:
            if "(Desc)" in sort_key:
                order_by_clause = "p.primary_name DESC"
            else:
                order_by_clause = "p.primary_name ASC"
        else:
            order_by_clause = "s.group_id ASC"

        # Calculate pagination
        offset = (page - 1) * page_size

        # Build SQL
        sql = f"""
        WITH base AS (
          SELECT
            group_id,
            account_name,
            is_primary,
            weakest_edge_to_primary,
            Disposition
          FROM read_parquet('{parquet_path}')
          WHERE {where_clause}
        ),
        stats AS (
          SELECT 
            group_id, 
            COUNT(*) AS group_size, 
            MAX(weakest_edge_to_primary) AS max_score
          FROM base
          GROUP BY group_id
        ),
        primary_names AS (
          SELECT
            group_id,
            any_value(account_name) FILTER (WHERE is_primary) AS primary_name
          FROM base
          GROUP BY group_id
        )
        SELECT
          s.group_id,
          s.group_size,
          s.max_score,
          COALESCE(p.primary_name, '') AS primary_name
        FROM stats s
        LEFT JOIN primary_names p USING (group_id)
        ORDER BY {order_by_clause}, s.group_id ASC
        LIMIT {page_size}
        OFFSET {offset};
        """

        query_build_time = time.time() - step_start

        logger.info(
            f'DuckDB query built | run_id={run_id} where_clause="{where_clause}" order_by="{order_by_clause}" elapsed={query_build_time:.3f}s'
        )

        check_timeout()

        # Step 3: Execute query
        step_start = time.time()
        result = conn.execute(sql)
        query_exec_time = time.time() - step_start

        logger.info(
            f"DuckDB query executed | run_id={run_id} elapsed={query_exec_time:.3f}s"
        )

        check_timeout()

        # Step 4: Convert to pandas
        step_start = time.time()
        df = result.df()
        pandas_time = time.time() - step_start

        logger.info(
            f"DuckDB pandas conversion | run_id={run_id} rows={len(df)} elapsed={pandas_time:.3f}s"
        )

        # Step 5: Convert to list of dicts
        page_data = df.to_dict("records")

        # Get total count
        count_sql = f"""
        WITH base AS (
          SELECT group_id
          FROM read_parquet('{parquet_path}')
          WHERE {where_clause}
        )
        SELECT COUNT(DISTINCT group_id) as total_groups
        FROM base;
        """
        total_result = conn.execute(count_sql)
        total_groups = total_result.fetchone()[0]

        # Close connection
        conn.close()

        elapsed = time.time() - start_time
        logger.info(
            f'DuckDB groups page loaded | run_id={run_id} rows={len(page_data)} offset={offset} sort="{sort_key}" elapsed={elapsed:.3f} projected_cols=["group_id", "group_size", "max_score", "primary_name"]'
        )

        return page_data, total_groups

    except PageFetchTimeout:
        elapsed = time.time() - start_time
        logger.error(
            f"DuckDB groups page load timeout | run_id={run_id} elapsed={elapsed:.3f}"
        )
        raise
    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(
            f'DuckDB groups page load failed | run_id={run_id} error="{str(e)}" elapsed={elapsed:.3f}'
        )
        return [], 0


def get_total_groups_count(run_id: str, filters: Dict[str, Any]) -> int:
    """
    Get total count of groups after applying filters.

    Args:
        run_id: Run ID to load data from
        filters: Dictionary of active filters

    Returns:
        Total number of groups
    """
    try:
        # Get artifact paths
        artifact_paths = get_artifact_paths(run_id)
        parquet_path = artifact_paths["review_ready_parquet"]

        if not os.path.exists(parquet_path):
            return 0

        # Use dataset scanning for efficient counting
        dataset = ds.dataset(parquet_path)

        # Project only group_id column for counting
        scanner = dataset.scanner(columns=["group_id"])
        projected_table = scanner.to_table()

        # Apply filters
        filtered_table = apply_filters_pyarrow(projected_table, filters)

        # Get unique group count efficiently
        # Note: This is still expensive but much better than loading all data
        unique_groups = filtered_table.column("group_id").unique()
        return int(len(unique_groups))

    except Exception as e:
        logger.error(f"Failed to get total groups count: {e}")
        return 0
