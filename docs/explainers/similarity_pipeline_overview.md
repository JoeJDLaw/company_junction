# Company Junction Similarity & Grouping Pipeline Overview

## Executive Summary

‚Ä¢ **Purpose**: Automatically group duplicate company records from Salesforce exports using name similarity
‚Ä¢ **Input**: CSV/Excel file with company names and account IDs
‚Ä¢ **Output**: Grouped records with similarity scores, group assignments, and merge recommendations
‚Ä¢ **Key Innovation**: "Soft-ban" blocking prevents hard exclusions of business names like "99 Cents" or "7-Eleven"
‚Ä¢ **Unit of Work**: Each `account_id` is a unique record; we deduplicate at the account level, not name level
‚Ä¢ **Performance**: Handles 100k+ records with bounded memory usage via sharding and capping
‚Ä¢ **Deterministic**: Same input always produces same output (no randomness)
‚Ä¢ **Configurable**: All thresholds, penalties, and blocking strategies are tunable via YAML

## What Changed vs Legacy

**üîÑ Soft-Ban Blocking (vs Hard-Ban)**
- **Before**: Common tokens like "99", "7", "24" were hard-banned, causing singleton groups
- **Now**: Denylist tokens are throttled (shard + prefilter + cap) but not eliminated
- **Result**: "99 Cents Only Stores" and "99 Cents Store" now generate candidate pairs
- **Architecture**: Modular design with separate `blocking.py`, `scoring.py`, and `diagnostics.py` modules

**üéØ Enhanced Normalization**
- **Before**: Basic normalization only (lowercase, punctuation removal)
- **Now**: Plural‚Üísingular mapping, weak token filtering, canonical retail terms
- **Result**: "stores" ‚Üí "store", "only" excluded from Jaccard, better retail brand matching

**üìä Deterministic Sharding**
- **Before**: Random or inconsistent sharding strategies
- **Now**: Configurable strategies (second_token, char_trigram, third_token_initial)
- **Result**: Reproducible results across runs

**üîç Diagnostics & Monitoring**
- **Before**: Limited visibility into blocking decisions
- **Now**: `block_stats.csv` and `brand_suggestions.csv` for analysis
- **Result**: Data-driven allowlist tuning and performance optimization
- **CLI Tools**: `scripts/inspect_blocking.py` with `--suggest-brands` and `--explain` options

## Step-by-Step Flow

1. **Load & Validate**: Read CSV/Excel file, validate required columns (`Account Name`, `Account ID`)
2. **Normalize Names**: Clean and standardize company names (`src/normalize.py`)
   - Remove punctuation, normalize case, extract legal suffixes
   - Create `name_core` (business name without suffix) and `suffix_class` (INC/LLC/etc.)
3. **Exact Equals Pass**: Group records with identical `name_core` values first
4. **Candidate Generation**: Find potential duplicate pairs using "soft-ban" blocking (`src/similarity/blocking.py`)
   - **Allowlisted bigram prepass**: Full pairing within matched bigram groups (e.g., "99 cents", "7 eleven")
   - **Allowlisted tokens** (99, 7, 24): Generate all candidate pairs
   - **Denylisted tokens** (the, and, of): Shard + prefilter + cap pairs
   - **Other tokens**: Standard blocking with sharding for large blocks
5. **Similarity Scoring**: Calculate similarity scores for candidate pairs using RapidFuzz
6. **Edge-Gating**: Apply thresholds to decide which pairs can form groups (`src/grouping.py`)
7. **Union-Find Grouping**: Build final groups using connected components algorithm
8. **Survivorship**: Select primary records and generate merge previews
9. **Disposition**: Apply business rules (Keep/Update/Delete/Verify)
10. **Output**: Generate review-ready files with group assignments and recommendations

## Key Concepts & Terms

**Account ID**: Unique identifier for each company record (our deduplication unit)
**Name Core**: Business name without legal suffix (e.g., "99 Cents Only Stores" from "99 Cents Only Stores LLC")
**Suffix Class**: Standardized legal entity type (INC, LLC, LTD, CORP, NONE)
**Blocking**: Strategy to reduce O(n¬≤) comparisons by grouping similar names first
**Candidate Pairs**: Potential duplicates identified during blocking phase
**Similarity Score**: 0-100 score combining multiple string similarity metrics
**Edge-Gating**: Rules determining if two accounts can be in the same group
**Union-Find**: Algorithm that builds connected groups from pairwise relationships
**Soft-Ban**: Throttling approach that shards + prefilters instead of hard-excluding tokens
**Allowlist**: Business-critical tokens (99, 7, 24) that must never be blocked
**Denylist**: True stopwords (the, and, of) that get throttled but not eliminated

## Example Walkthrough

### "99 Cents Only Stores" vs "99 Cents Store"

**Step 1: Normalization**
- Input: "99 Cents Only Stores LLC" ‚Üí `name_core`: "99 cents only stores", `suffix_class`: "LLC"
- Input: "99 Cents Store Inc" ‚Üí `name_core`: "99 cents store", `suffix_class`: "INC"

**Step 2: Blocking (Soft-Ban)**
- First token: "99" (allowlisted)
- Strategy: Generate all candidate pairs (no throttling)
- Result: Pair created between the two records

**Step 3: Enhanced Normalization**
```python
# From src/normalize.py:enhance_name_core()
enhanced_a = "99 cents only store"  # stores ‚Üí store (plural normalization)
enhanced_b = "99 cents store"
weak_tokens = {"only"}  # "only" is weak token, excluded from Jaccard
jaccard_tokens_a = {"99", "cents", "store"}  # excluding weak tokens
jaccard_tokens_b = {"99", "cents", "store"}
```

**Step 4: Similarity Scoring**
```python
# From src/similarity/scoring.py:compute_score_components()
ratio_name = fuzz.token_sort_ratio("99 cents only store", "99 cents store")  # ~85
ratio_set = fuzz.token_set_ratio("99 cents only store", "99 cents store")    # 100
jaccard = len({"99", "cents", "store"} & {"99", "cents", "store"}) / 
          len({"99", "cents", "store"} | {"99", "cents", "store"})           # 1.0
base = 0.45 * 85 + 0.35 * 100 + 20.0 * 1.0  # ~93
# Apply suffix mismatch penalty: -25
score = 68  # With different suffixes (LLC vs INC)
# OR score = 93  # With matching suffixes (LLC vs LLC)
```

**Step 5: Edge-Gating**
- Score: 68 (with suffix penalty) or 93 (without suffix penalty)
- With matching suffixes: 93 ‚â• 92 ‚Üí **Direct grouping** under high threshold
- With different suffixes: 68 < 84 ‚Üí **No grouping** (would need medium+shared-token rule)

**Step 6: Group Assignment**
- `group_id`: Generated from sorted account IDs
- `group_size`: 2
- `weakest_edge_to_primary`: 74
- `group_join_reason`: "edge>=medium+shared_token"

### "7-Eleven" Example

**Multiple 7-Eleven locations**:
- "7-Eleven Store #123" ‚Üí `name_core`: "7 eleven store 123"
- "7-Eleven Store #456" ‚Üí `name_core`: "7 eleven store 456"
- "7 Eleven Inc" ‚Üí `name_core`: "7 eleven"

**Blocking**: All have first token "7" (allowlisted) ‚Üí all pairs generated
**Scoring**: High similarity scores (85-95) due to shared "7 eleven" prefix
**Grouping**: All locations group together under high threshold rule

## Mermaid Pipeline Diagram

```mermaid
graph TD
    A[Load CSV/Excel] --> B[Normalize Names]
    B --> C[Exact Equals Pass]
    C --> D[Soft-Ban Blocking]
    
    D --> E{Token Type?}
    E -->|Allowlisted| F[Generate All Pairs]
    E -->|Denylisted| G[Shard + Prefilter + Cap]
    E -->|Other| H[Standard Blocking]
    
    F --> I[Similarity Scoring]
    G --> I
    H --> I
    
    I --> J[Edge-Gating Rules]
    J --> K{Score ‚â• 92?}
    K -->|Yes| L[High Threshold Join]
    K -->|No| M{Score ‚â• 84 + Shared Tokens?}
    M -->|Yes| N[Medium+Shared Join]
    M -->|No| O[No Join]
    
    L --> P[Union-Find Grouping]
    N --> P
    O --> P
    
    P --> Q[Survivorship Selection]
    Q --> R[Disposition Rules]
    R --> S[Output Files]
```

## Developer Appendix

### Core Files & Functions

**`src/cleaning.py`**
- `run_pipeline()`: Main orchestration function
- `load_salesforce_data()`: File loading and validation
- Pipeline stages: normalization ‚Üí filtering ‚Üí exact_equals ‚Üí candidate_generation ‚Üí grouping ‚Üí survivorship ‚Üí disposition

**`src/normalize.py`**
- `normalize_name()`: Core normalization function
- `normalize_dataframe()`: Batch normalization for DataFrames
- `enhance_name_core()`: Enhanced normalization with plural‚Üísingular, weak tokens, canonical terms
- `get_enhanced_tokens_for_jaccard()`: Token sets excluding weak tokens for Jaccard calculation
- `_create_name_base()`: Symbol mapping and cleaning
- `extract_suffix()`: Legal suffix detection
- Outputs: `name_core`, `suffix_class`, `name_base`, `alias_candidates`

**`src/similarity/` (Modular Structure)**
- `__init__.py`: Main orchestration and `pair_scores()` entry point
- `blocking.py`: `generate_candidate_pairs_soft_ban()`, `get_stop_tokens()`, sharding logic
- `scoring.py`: `score_pairs_parallel()`, `score_pairs_bulk()`, `compute_score_components()`
- `diagnostics.py`: `write_blocking_diagnostics()`, `generate_brand_suggestions()`

**`src/grouping.py`**
- `create_groups_with_edge_gating()`: Main grouping function
- `can_join_group()`: Edge-gating decision logic
- `apply_canopy_bound()`: Group size limits
- Union-Find implementation for connected components

### Configuration Keys

**`config/settings.yaml`**

```yaml
similarity:
  high: 92                    # High threshold for direct grouping (src/grouping.py:76)
  medium: 84                  # Medium threshold (src/grouping.py:77)
  penalty:
    suffix_mismatch: 25       # Penalty for different legal suffixes (src/similarity/scoring.py)
    num_style_mismatch: 5     # Penalty for numeric style differences (src/similarity/scoring.py)
    punctuation_mismatch: 3   # Penalty for punctuation differences (src/similarity/scoring.py)
  
  blocking:
    allowlist_tokens: ["99", "7", "24", ...]  # Never block these (src/similarity/blocking.py)
    allowlist_bigrams: ["99 cents", "7 eleven", ...]  # Never block these (src/similarity/blocking.py)
    denylist_tokens: ["the", "and", "of", ...]  # Throttle these (src/similarity/blocking.py)
    stop_tokens: ["inc", "llc", "ltd"]  # Skip in first-token blocking (src/similarity/blocking.py)
    soft_ban:
      shard_strategy: "second_token"     # How to shard large blocks (src/similarity/blocking.py)
      fallback_shard: "char_trigram"    # Fallback sharding strategy (src/similarity/blocking.py)
      max_shard_size: 200               # Max records per shard (src/similarity/blocking.py)
      char_bigram_gate: 0.1             # Min char bigram overlap (src/similarity/blocking.py)
      length_window: 10                 # Max length difference (src/similarity/blocking.py)
      min_token_overlap: 1              # Min shared tokens (src/similarity/blocking.py)
      max_candidates_per_record: 50     # Cap pairs per record (src/similarity/blocking.py)
      block_cap: 800                    # Max block size before sharding (src/similarity/blocking.py)
  
  scoring:
    use_bulk_cdist: true                # Use bulk scoring for performance (src/similarity/scoring.py)
    gate_cutoff: 72                     # Minimum score for bulk gating (src/similarity/scoring.py)
  
  normalization:
    weak_tokens: ["only", "the", "and", ...]  # Exclude from Jaccard (src/normalize.py:562)
    plural_singular_map: {stores: store, ...}  # Plural normalization (src/normalize.py:558)
    canonical_retail_terms: {store: store, ...}  # Retail term mapping (src/normalize.py:555)
    enable_plural_normalization: true   # Enable plural‚Üísingular (src/normalize.py:543)
    enable_weak_token_filtering: true   # Enable weak token filtering (src/normalize.py:542)
    enable_canonical_retail_terms: true # Enable canonical terms (src/normalize.py:544)

grouping:
  edge_gating:
    allow_medium_plus_shared_token: true  # Enable medium+shared rule (src/grouping.py:84)
```

### Similarity Scoring Details

**Score Components** (`src/similarity/scoring.py:compute_score_components()`):
```python
ratio_name = fuzz.token_sort_ratio(name_core_a, name_core_b)  # Token order insensitive
ratio_set = fuzz.token_set_ratio(name_core_a, name_core_b)    # Token set overlap
jaccard = len(tokens_a & tokens_b) / len(tokens_a | tokens_b)  # Token overlap ratio

base = 0.45 * ratio_name + 0.35 * ratio_set + 20.0 * jaccard

# Apply penalties
if not suffix_match: base -= 25
if not num_style_match: base -= 5  
if punctuation_mismatch: base -= 3

score = max(0, min(100, round(base)))
```

### Edge-Gating Rules

**High Threshold** (`src/grouping.py:can_join_group()`):
- Score ‚â• 92 ‚Üí Direct grouping
- Reason: "edge>=high"

**Medium + Shared Token**:
- Score ‚â• 84 AND shared tokens > 0 ‚Üí Grouping
- Reason: "edge>=medium+shared_token"
- Shared tokens exclude stop words (inc, llc, ltd)

**Canopy Bound**:
- Limits group size to prevent mega-groups
- Applied after edge-gating decisions

### Soft-Ban Implementation

**Allowlisted Tokens** (99, 7, 24, etc.):
- Generate all candidate pairs within block
- No throttling or prefiltering
- Preserves recall for business names

**Denylisted Tokens** (the, and, of, etc.):
- Shard by second token or character trigrams
- Apply prefiltering gates within each shard:
  - Length window: |len(a) - len(b)| ‚â§ 10
  - Token overlap: ‚â• 1 shared token
  - Char bigram overlap: ‚â• 0.1 ratio
- Cap at 50 candidates per record
- Result: Throttled but not eliminated

### Diagnostics & Monitoring

**Block Statistics** (`data/interim/block_stats.csv`):
- `token`: First token
- `count`: Number of records
- `strategy`: soft_ban_sharded | standard_sharded | full_pairs
- `pairs_generated`: Number of candidate pairs
- `pairs_capped`: Number of pairs capped by limits

**Brand Suggestions** (`data/interim/brand_suggestions.csv`):
- Placeholder for future adaptive suggestions
- Will identify tokens that might belong in allowlist

### Performance Controls

**Block Cap**: 800 records max per block before sharding
**Candidate Cap**: 50 pairs max per record in soft-ban mode
**Sharding Strategies**:
- `second_token`: Group by second word
- `char_trigram`: Group by first 3 characters
- `third_token_initial`: Group by third word's first letter

## Upgrade Notes

**Breaking Changes in v2.0:**
- **Modular Architecture**: `src/similarity.py` split into `src/similarity/` package with separate modules
- **Legacy blocking removed**: Only soft-ban blocking strategy is supported
- **Config migration**: Old `similarity.performance.*` settings replaced with `similarity.blocking.*` and `similarity.scoring.*`

**Migration Steps:**
1. Update imports: `from src.similarity import pair_scores` (unchanged)
2. Remove any references to legacy `similarity.performance` config keys
3. Update custom scripts that imported from `src.similarity` (now use submodules)
4. Run `scripts/inspect_blocking.py --suggest-brands` to get allowlist recommendations

**New Features:**
- **Modular Design**: Separate `blocking.py`, `scoring.py`, and `diagnostics.py` modules
- **Enhanced CLI**: `scripts/score_pair.py` for detailed similarity traces
- **Brand Suggestions**: `data/interim/brand_suggestions.csv` with confidence scores
- **Improved Normalization**: Plural‚Üísingular mapping and weak token filtering
- **Comprehensive Testing**: 30+ unit tests covering all scenarios

## FAQ

**Q: Do we score every row against every other row?**
A: No. We use blocking to reduce O(n¬≤) to O(n log n). Only records with similar first tokens are compared.

**Q: Why didn't my "99 Cents" records group together?**
A: Check if "99" is in the allowlist. With soft-ban, they should generate candidate pairs and group if similarity ‚â• 84 with shared tokens.

**Q: What does "medium+shared-token" require?**
A: Similarity score ‚â• 84 AND at least one shared token (excluding stop words like "inc", "llc").

**Q: How do I add a token to the allowlist?**
A: Edit `config/settings.yaml` ‚Üí `similarity.blocking.allowlist_tokens` ‚Üí add your token ‚Üí restart pipeline.

**Q: What's the difference between denylist and disposition blacklist?**
A: Denylist throttles candidate generation (the, and, of). Disposition blacklist marks records as "Delete" in UI (test, sample, unknown).

**Q: Why are some groups still singletons after the fix?**
A: Check similarity scores. Records need ‚â• 84 score + shared tokens to group. Very different names won't group even with same first token.

**Q: How do I see what tokens are being processed?**
A: Check `data/interim/block_stats.csv` after running the pipeline.

**Q: How do I get brand suggestions?**
A: Run `scripts/inspect_blocking.py --suggest-brands` or check `data/interim/brand_suggestions.csv`.

**Q: Can I disable soft-ban and go back to legacy behavior?**
A: No. Legacy blocking has been removed. Only soft-ban strategy is supported.

## Implementation Status

### ‚úÖ Completed Features

1. **Modular Architecture**: Complete refactoring into `blocking.py`, `scoring.py`, and `diagnostics.py` modules
2. **Soft-Ban Blocking**: Fully implemented with allowlist/denylist support and safety rails
3. **Enhanced Normalization**: Plural‚Üísingular mapping, weak token filtering, canonical retail terms
4. **Comprehensive Testing**: 30+ unit tests covering all major scenarios and edge cases
5. **CLI Tools**: `scripts/score_pair.py` and `scripts/inspect_blocking.py` for debugging and analysis
6. **Diagnostics**: Block statistics and brand suggestions with proper file output
7. **Performance Optimization**: Bulk scoring, sharding, and candidate capping
8. **Configuration**: Complete YAML schema with all blocking, scoring, and normalization settings

### üîÑ Future Enhancements

1. **Brand Suggestions Algorithm**: Currently uses placeholder heuristic - could be enhanced with actual singleton rate analysis
2. **Performance Monitoring**: Could add timing metrics and memory usage tracking
3. **A/B Testing**: Could add framework for comparing different blocking strategies
4. **Advanced Sharding**: Could add more sophisticated sharding strategies for very large datasets

### CLI Tools Available

```bash
# Score a specific pair with detailed trace
python scripts/score_pair.py "99 Cents Only Stores LLC" "99 Cents Store LLC" --suffix-a "LLC" --suffix-b "LLC"

# Inspect blocking behavior and get brand suggestions
python scripts/inspect_blocking.py --suggest-brands --explain "99" --export-suggestions
```

These tools provide:
- Detailed similarity scoring traces
- Blocking behavior analysis
- Brand suggestion recommendations
- Performance impact metrics
