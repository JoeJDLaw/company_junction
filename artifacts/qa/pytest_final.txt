.........F...............ss.............................Essssssssss..... [  9%]
..F................................................................FFFFF [ 19%]
F.............F.FF........FF.................................FF.F.FF.... [ 29%]
.....................................................................F.. [ 39%]
..................F..................................................... [ 48%]
........................................F.FF........................sss. [ 58%]
........................................................................ [ 68%]
........................................................................ [ 78%]
........................................................................ [ 87%]
..F..................................................................... [ 97%]
.................                                                        [100%]
==================================== ERRORS ====================================
_______________ ERROR at setup of test_latest_pointer_operations _______________
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_utils.py, line 154
  def test_latest_pointer_operations(sample_runs, cache_utils_workspace):
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_utils.py, line 22
  @pytest.fixture
  def sample_runs(cache_utils_workspace):
E       fixture 'cache_utils_workspace' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, failed_runs, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_runs, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_utils.py:22
=================================== FAILURES ===================================
_______ TestAliasMatchingParallelism.test_parallel_executor_integration ________

self = <Mock name='mock.should_use_parallel' id='4745312208'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'should_use_parallel' to have been called once. Called 2 times.
E           Calls: [call(5), call(5)].

/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/unittest/mock.py:923: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.test_alias_matching_parallelism.TestAliasMatchingParallelism object at 0x11a844da0>

    def test_parallel_executor_integration(self):
        """Test that ParallelExecutor is properly integrated."""
        # Create a mock ParallelExecutor
        mock_executor = Mock(spec=ParallelExecutor)
        mock_executor.should_use_parallel.side_effect = lambda x: True
        mock_executor.workers = 2
        mock_executor.execute_chunked.return_value = [
            [
                {
                    "record_id": 100,
                    "alias_text": "Acme Corp",
                    "match_record_id": 101,
                    "match_group_id": "G1",
                    "score": 95,
                    "suffix_match": True,
                },
            ],
            [
                {
                    "record_id": 101,
                    "alias_text": "Acme Corp",
                    "match_record_id": 100,
                    "match_group_id": "G1",
                    "score": 95,
                    "suffix_match": True,
                },
            ],
        ]
    
        # Call compute_alias_matches with ParallelExecutor
        result_df, stats = compute_alias_matches(
            self.df_norm,
            self.df_groups,
            self.settings,
            parallel_executor=mock_executor,
        )
    
        # Verify ParallelExecutor was used
>       mock_executor.should_use_parallel.assert_called_once()
E       AssertionError: Expected 'should_use_parallel' to have been called once. Called 2 times.
E       Calls: [call(5), call(5)].
E       
E       pytest introspection follows:
E       
E       Args:
E       assert (5,) == ()
E         
E         Left contains one more item: 5
E         Use -v to get more diff

tests/test_alias_matching_parallelism.py:107: AssertionError
_________________ TestCleaning.test_validate_required_columns __________________

self = <tests.test_cleaning.TestCleaning testMethod=test_validate_required_columns>

    def test_validate_required_columns(self) -> None:
        """Test column validation logic."""
        # Test with all required columns
>       self.assertTrue(validate_required_columns(self.sample_data))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_cleaning.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df =   Account ID    Account Name         Relationship Created Date
0        001       Acme Corp  Other/Miscellaneous   202...   003  Tech Solutions  Other/Miscellaneous   2021-01-03
3        004  Tech Solutions  Other/Miscellaneous   2021-01-04

    def validate_required_columns(df: pd.DataFrame) -> bool:
        """Validate that required columns are present.
    
        Args:
            df: DataFrame to validate
    
        Returns:
            True if validation passes
    
        Raises:
            ValueError: If required columns are missing
    
        """
        # Use canonical column names since DataFrame has been renamed
        # Only check for columns that are actually mapped and renamed
        required_columns = [ACCOUNT_ID, ACCOUNT_NAME, CREATED_DATE]
    
        # Check for Account Name (required)
        if ACCOUNT_NAME not in df.columns:
>           raise ValueError(f"Missing required name column: {ACCOUNT_NAME}")
E           ValueError: Missing required name column: account_name

src/cleaning.py:322: ValueError
_______________ TestBuildCliCommand.test_build_cli_command_basic _______________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x11a96d2e0>

    def test_build_cli_command_basic(self) -> None:
        """Test basic command building."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...settings.yaml' == 'python src/c...settings.yaml'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml
E         ?        ------------------------
E         + est.csv --config config/settings.yaml

tests/test_cli_builder.py:207: AssertionError
_________ TestBuildCliCommand.test_build_cli_command_with_parallelism __________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x11a96d520>

    def test_build_cli_command_with_parallelism(self) -> None:
        """Test command with parallelism flags."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            workers=4,
            parallel_backend="threading",
            chunk_size=1000,
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --workers 4 --parallel-backend threading --chunk-size 1000"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...unk-size 1000' == 'python src/c...unk-size 1000'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --workers 4 --parallel-backend threading --chunk-size 1000
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --workers 4 --parallel-backend threading --chunk-size 1000

tests/test_cli_builder.py:219: AssertionError
____________ TestBuildCliCommand.test_build_cli_command_no_parallel ____________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x11a96d6a0>

    def test_build_cli_command_no_parallel(self) -> None:
        """Test command with no-parallel flag."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            no_parallel=True,
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --no-parallel"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...--no-parallel' == 'python src/c...--no-parallel'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --no-parallel
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --no-parallel

tests/test_cli_builder.py:229: AssertionError
_________ TestBuildCliCommand.test_build_cli_command_with_run_control __________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x11a96d820>

    def test_build_cli_command_with_run_control(self) -> None:
        """Test command with run control flags."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            no_resume=True,
            run_id="custom_run_123",
            keep_runs=5,
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --no-resume --run-id custom_run_123 --keep-runs 5"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...--keep-runs 5' == 'python src/c...--keep-runs 5'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --no-resume --run-id custom_run_123 --keep-runs 5
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --no-resume --run-id custom_run_123 --keep-runs 5

tests/test_cli_builder.py:241: AssertionError
__________ TestBuildCliCommand.test_build_cli_command_with_extra_args __________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x11a96d9a0>

    def test_build_cli_command_with_extra_args(self) -> None:
        """Test command with extra arguments."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            extra_args="--verbose --debug",
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --verbose --debug"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...rbose --debug' == 'python src/c...rbose --debug'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --verbose --debug
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --verbose --debug

tests/test_cli_builder.py:251: AssertionError
___________ TestBuildCliCommand.test_build_cli_command_custom_outdir ___________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x11a96db20>

    def test_build_cli_command_custom_outdir(self) -> None:
        """Test command with custom output directory."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            outdir="custom/output",
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir custom/output --config config/settings.yaml"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...custom/output' == 'python src/c...settings.yaml'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir custom/output --config config/settings.yaml
E         + est.csv --config config/settings.yaml --outdir custom/output

tests/test_cli_builder.py:261: AssertionError
_____ TestGroupDetailsDuckDB.test_get_group_details_duckdb_file_not_found ______

self = <tests.test_details_fast_path.TestGroupDetailsDuckDB object at 0x11a96fe30>
mock_exists = <MagicMock name='exists' id='4746433344'>
mock_get_paths = <MagicMock name='get_artifact_paths' id='4747491808'>

    @patch("src.utils.artifact_management.get_artifact_paths")
    @patch("os.path.exists")
    def test_get_group_details_duckdb_file_not_found(
        self,
        mock_exists: MagicMock,
        mock_get_paths: MagicMock,
    ) -> None:
        """Test DuckDB details loading when file not found."""
        # Mock artifact paths
        mock_get_paths.return_value = {
            "group_details_parquet": "/test/path/group_details.parquet",
        }
        mock_exists.return_value = False
    
        # Test the function should raise FileNotFoundError
        with pytest.raises(FileNotFoundError):
>           get_group_details_duckdb(
                "/test/path/group_details.parquet",
                "group1",
                "account_id",
                1,
                10,
                {},
                {},
            )

tests/test_details_fast_path.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

parquet_path = '/test/path/group_details.parquet', group_id = 'group1'
order_by = 'account_id', page = 1, page_size = 10, filters = {}, settings = {}

    def _get_group_details_duckdb(
        parquet_path: str,
        group_id: str,
        order_by: str,
        page: int,
        page_size: int,
        filters: Dict[str, Any],
        settings: Dict[str, Any],
    ) -> Tuple[List[Dict[str, Any]], int]:
        """DuckDB backend for group details (fast filtering + pagination)."""
        if DUCKDB is None:
            raise ImportError("DuckDB not available for group details")
    
        duckdb_threads = settings.get("ui", {}).get("duckdb_threads", 4)
        timeout_seconds = settings.get("ui", {}).get("timeout_seconds", 30)
    
        start = time.time()
    
        def check_timeout() -> None:
            if time.time() - start > timeout_seconds:
                raise DetailsFetchTimeout(f"Exceeded {timeout_seconds}s")
    
        # Get available columns dynamically
        available_columns = _get_available_columns(parquet_path)
        dynamic_select = _build_dynamic_select(available_columns)
    
        where_clause, params = _build_where_clause(filters, available_columns)
        # Clamp pagination inputs to avoid negative offsets and cap for performance
        page = max(1, int(page))
        requested_size = int(page_size)
        max_page_size = settings.get("ui", {}).get("max_page_size", 250)
        page_size = max(1, min(requested_size, max_page_size))
        if page_size != requested_size:  # Log when clamping occurs
            logger.info(
                "Page size clamped from %s to %s (max_page_size limit)",
                requested_size,
                max_page_size,
            )
            record_page_size_clamped()
        offset = (page - 1) * page_size
    
        # Build SQL using dynamic column selection
        sql = (
            dynamic_select + "WHERE " + GROUP_ID + " = ? AND " + where_clause + " "
            "ORDER BY "
            + order_by
            + " NULLS LAST, "
            + ACCOUNT_NAME
            + " ASC "  # order_by from get_order_by whitelist, stable tie-breaker, NULLs last
            "LIMIT ? OFFSET ?"
        )
        params_page = [parquet_path, group_id, *params, page_size, offset]
    
        count_sql = (
            "SELECT COUNT(*) FROM read_parquet(?) "
            "WHERE " + GROUP_ID + " = ? AND " + where_clause
        )
        params_count = [parquet_path, group_id, *params]
    
        conn = None
        try:
            conn = DUCKDB.connect(":memory:")
            duckdb_threads = int(duckdb_threads or 4)  # Ensure numeric
            duckdb_threads = min(duckdb_threads, 32)  # Double-enforce caps at call site
            conn.execute("PRAGMA threads=" + str(duckdb_threads))
            check_timeout()
    
>           res = conn.execute(sql, params_page)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           duckdb.duckdb.IOException: IO Error: No files found that match the pattern "/test/path/group_details.parquet"

src/utils/group_details.py:398: IOException
------------------------------ Captured log call -------------------------------
WARNING  src.utils.group_details:group_details.py:96 Failed to read schema from /test/path/group_details.parquet: [Errno 2] Failed to open local file '/test/path/group_details.parquet'. Detail: [errno 2] No such file or directory
______ TestGroupsRouting.test_groups_use_duckdb_when_stats_parquet_exists ______

self = <tests.test_details_fast_path.TestGroupsRouting object at 0x11a99c170>
mock_exists = <MagicMock name='exists' id='4749829824'>
mock_get_paths = <MagicMock name='get_artifact_paths' id='4749833712'>

    @patch("src.utils.artifact_management.get_artifact_paths")
    @patch("os.path.exists")
    @patch("src.utils.opt_deps.DUCKDB_AVAILABLE", True)
    def test_groups_use_duckdb_when_stats_parquet_exists(
        self,
        mock_exists: MagicMock,
        mock_get_paths: MagicMock,
    ) -> None:
        """Test that groups page uses DuckDB when group_stats.parquet exists."""
        from src.utils.group_pagination import get_groups_page
    
        # Mock artifact paths with group_stats.parquet
        mock_get_paths.return_value = {
            "group_stats_parquet": "/test/path/group_stats.parquet",
        }
        mock_exists.return_value = True
    
        # Mock the DuckDB function to return test data
>       with patch(
            "src.utils.ui_helpers.get_groups_page_from_stats_duckdb",
        ) as mock_duckdb:

tests/test_details_fast_path.py:331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/unittest/mock.py:1439: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'src.utils.ui_helpers'

    def resolve_name(name):
        """
        Resolve a name to an object.
    
        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:
    
        W(.W)*
        W(.W)*:(W(.W)*)?
    
        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.
    
        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.
    
        The function will return an object (which might be a module), or raise one
        of the following exceptions:
    
        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)
    
        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'src.utils' has no attribute 'ui_helpers'

/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/pkgutil.py:528: AttributeError
__ TestGroupsRouting.test_groups_fallback_to_pyarrow_when_duckdb_unavailable ___

self = <tests.test_details_fast_path.TestGroupsRouting object at 0x11a99c2c0>
mock_exists = <MagicMock name='exists' id='4749806384'>
mock_get_paths = <MagicMock name='get_artifact_paths' id='4749811184'>

    @patch("src.utils.artifact_management.get_artifact_paths")
    @patch("os.path.exists")
    @patch("src.utils.opt_deps.DUCKDB_AVAILABLE", False)
    def test_groups_fallback_to_pyarrow_when_duckdb_unavailable(
        self,
        mock_exists: MagicMock,
        mock_get_paths: MagicMock,
    ) -> None:
        """Test that groups page falls back to PyArrow when DuckDB unavailable."""
        from src.utils.group_pagination import get_groups_page
    
        # Mock artifact paths with group_stats.parquet
        mock_get_paths.return_value = {
            "group_stats_parquet": "/test/path/group_stats.parquet",
        }
        mock_exists.return_value = True
    
        # Mock the PyArrow function to return test data
>       with patch("src.utils.ui_helpers.get_groups_page_pyarrow") as mock_pyarrow:

tests/test_details_fast_path.py:362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/unittest/mock.py:1439: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'src.utils.ui_helpers'

    def resolve_name(name):
        """
        Resolve a name to an object.
    
        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:
    
        W(.W)*
        W(.W)*:(W(.W)*)?
    
        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.
    
        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.
    
        The function will return an object (which might be a module), or raise one
        of the following exceptions:
    
        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)
    
        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'src.utils' has no attribute 'ui_helpers'

/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/pkgutil.py:528: AttributeError
_______________ TestDisposition.test_manual_override_application _______________

self = <tests.test_disposition.TestDisposition testMethod=test_manual_override_application>

    def test_manual_override_application(self) -> None:
        """Test that manual overrides are applied correctly."""
        import json
        from pathlib import Path
    
        # Create test data
        test_data = pd.DataFrame(
            {
                "group_id": [1, 1],
                "Account Name": ["Acme Corp Inc", "Acme Corp Inc"],
                "is_primary": [True, False],
                "weakest_edge_to_primary": [100, 95],
                "suffix_class": ["INC", "INC"],
                "has_multiple_names": [False, False],
            },
        )
    
        # Create manual override file
        manual_dir = Path("data/manual")
        manual_dir.mkdir(parents=True, exist_ok=True)
    
        override_data = [
            {
                "record_id": "0",  # First record
                "account_id": "001Hs000054S8kI",
                "account_name": "Acme Corp Inc",
                "name_core": "acme corp",
                "override": "Delete",
                "reason": "Test override",
                "ts": "2024-01-01T00:00:00",
            },
        ]
    
        with open(manual_dir / "manual_dispositions.json", "w") as f:
            json.dump(override_data, f)
    
        try:
            # Apply dispositions
            result = apply_dispositions(test_data, self.settings)
    
            # Check that manual override was applied
            self.assertEqual(result.iloc[0]["disposition"], "Delete")
>           self.assertEqual(
                result.iloc[0]["disposition_reason"],
                "manual_override:Delete",
            )
E           AssertionError: 'primary_record' != 'manual_override:Delete'
E           - primary_record
E           + manual_override:Delete

tests/test_disposition.py:301: AssertionError
_______________ TestDisposition.test_multiple_names_verification _______________

self = <tests.test_disposition.TestDisposition testMethod=test_multiple_names_verification>

    def test_multiple_names_verification(self) -> None:
        """Test that records with multiple names are marked as Verify."""
        # Create test data with multiple names
        df_groups = self.df_norm.copy()
        df_groups["group_id"] = [0, 1, 2, 3, 4, 5, 6, 7]
        df_groups["is_primary"] = [True] * 8
        df_groups["weakest_edge_to_primary"] = [0.0] * 8
    
        # Add multiple names flag
        df_groups.loc[0, "has_multiple_names"] = True
        df_groups.loc[1, "has_multiple_names"] = True
    
        df_dispositions = apply_dispositions(df_groups, self.settings)
    
        # Check that multiple names are marked as Verify
        self.assertEqual(df_dispositions.iloc[0]["disposition"], "Verify")
        self.assertEqual(df_dispositions.iloc[1]["disposition"], "Verify")
>       self.assertEqual(
            df_dispositions.iloc[0]["disposition_reason"],
            "multi_name_string_requires_split",
        )
E       AssertionError: 'clean_singleton' != 'multi_name_string_requires_split'
E       - clean_singleton
E       + multi_name_string_requires_split

tests/test_disposition.py:254: AssertionError
___________________________ test_duckdb_memoization ____________________________

    def test_duckdb_memoization() -> None:
        """Test that memoization works correctly."""
        df = create_test_group_data()
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            settings = {
                "engine": {"duckdb": {"threads": 2}},
                "io": {"parquet": {"compression": "zstd"}},
>               "group_stats": {"memoization": {"enable": True}, "cache_dir": temp_dir},
                                                                              ^^^^^^^^
            }
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:154: NameError
__________________________ test_duckdb_parquet_write ___________________________

    def test_duckdb_parquet_write() -> None:
        """Test that DuckDB can write optimized Parquet files."""
        df = create_test_group_data()
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            settings = {
                "engine": {"duckdb": {"threads": 2}},
                "io": {"parquet": {"compression": "zstd"}},
>               "group_stats": {"memoization": {"enable": True}, "cache_dir": temp_dir},
                                                                              ^^^^^^^^
            }
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:200: NameError
__________________________ test_parquet_size_reporter __________________________

    def test_parquet_size_reporter() -> None:
        """Test that parquet size reporter works correctly."""
        df = create_test_group_data()
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            # Create test parquet file
>           test_path = f"{temp_dir}/test.parquet"
                           ^^^^^^^^
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:286: NameError
_________________________ test_performance_improvement _________________________

    def test_performance_improvement() -> None:
        """Test that DuckDB is faster than pandas for group stats."""
        # Create larger test dataset
        n_records = 1000
        n_groups = 100
    
        # Ensure each group has exactly one primary record
        group_primary_map = {}
        is_primary_list = []
    
        for i in range(n_records):
            group_id = f"G{i % n_groups}"
            if group_id not in group_primary_map:
                group_primary_map[group_id] = i
                is_primary_list.append(True)
            else:
                is_primary_list.append(False)
    
        df_large = pd.DataFrame(
            {
                "group_id": [f"G{i % n_groups}" for i in range(n_records)],
                "account_id": [f"A{i}" for i in range(n_records)],
                "account_name": [f"Company {i % n_groups}" for i in range(n_records)],
                "is_primary": is_primary_list,
                "weakest_edge_to_primary": [
                    np.random.uniform(80, 100) for _ in range(n_records)
                ],
                "disposition": [
                    "Keep" if is_primary else "Update" for is_primary in is_primary_list
                ],
            },
        )
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            settings = {
                "engine": {"duckdb": {"threads": 2}},
                "io": {"parquet": {"compression": "zstd"}},
>               "group_stats": {"memoization": {"enable": True}, "cache_dir": temp_dir},
                                                                              ^^^^^^^^
            }
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:348: NameError
_________________________ test_no_f_string_sql_queries _________________________

    def test_no_f_string_sql_queries():
        """Verify no f-string SQL queries exist in group-related modules.
    
        This test ensures all DuckDB queries use parameterized placeholders
        instead of string interpolation to prevent SQL injection.
        """
        # Files to check for SQL queries
        sql_files = [
            "src/utils/group_pagination.py",
            "src/utils/group_details.py",
            "src/utils/group_stats.py",
            "src/utils/filtering.py",
        ]
    
        violations = []
    
        for file_path in sql_files:
            if Path(file_path).exists():
                f_strings = find_string_interpolations(file_path)
                if f_strings:
                    violations.append(
                        f"{file_path}: {len(f_strings)} f-string SQL queries found",
                    )
    
        if violations:
>           pytest.fail(
                "Found f-string SQL queries. All DuckDB queries must use parameterized placeholders:\n"
                + "\n".join(violations)
                + "\n\nExample of correct usage:\n"
                + "query = 'SELECT * FROM groups WHERE run_id = ?'\n"
                + "result = conn.execute(query, [run_id]).fetchdf()",
            )
E           Failed: Found f-string SQL queries. All DuckDB queries must use parameterized placeholders:
E           src/utils/group_pagination.py: 1 f-string SQL queries found
E           src/utils/group_details.py: 2 f-string SQL queries found
E           
E           Example of correct usage:
E           query = 'SELECT * FROM groups WHERE run_id = ?'
E           result = conn.execute(query, [run_id]).fetchdf()

tests/test_duckdb_query_params.py:170: Failed
____________________________ test_absolute_imports _____________________________

    def test_absolute_imports() -> None:
        """Test that all modules can be imported using absolute imports."""
        modules_to_test = [
            # Core modules
            "src.utils.cache_utils",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.io_utils",
            "src.utils.logging_utils",
            "src.utils.parallel_utils",
            "src.utils.path_utils",
            "src.utils.perf_utils",
            "src.utils.resource_monitor",
            "src.utils.sort_utils",
            "src.utils.state_utils",
            "src.utils.validation_utils",
            "src.utils.fragment_utils",  # Phase 1.18.3 addition
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
            "src.alias_matching",
            "src.cleaning",
            "src.disposition",
            "src.grouping",
            "src.manual_io",
            "src.normalize",
            "src.performance",
            "src.salesforce",
            "src.similarity",
            "src.survivorship",
            # App modules
            "app.components.controls",
            "app.components.export",
            "app.components.group_details",
            "app.components.group_list",
            "app.components.maintenance",
            "app.main",
        ]
    
        for module_name in modules_to_test:
            try:
>               module = __import__(module_name, fromlist=[""])
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E               ModuleNotFoundError: No module named 'src.salesforce'

tests/test_imports.py:49: ModuleNotFoundError

During handling of the above exception, another exception occurred:

    def test_absolute_imports() -> None:
        """Test that all modules can be imported using absolute imports."""
        modules_to_test = [
            # Core modules
            "src.utils.cache_utils",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.io_utils",
            "src.utils.logging_utils",
            "src.utils.parallel_utils",
            "src.utils.path_utils",
            "src.utils.perf_utils",
            "src.utils.resource_monitor",
            "src.utils.sort_utils",
            "src.utils.state_utils",
            "src.utils.validation_utils",
            "src.utils.fragment_utils",  # Phase 1.18.3 addition
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
            "src.alias_matching",
            "src.cleaning",
            "src.disposition",
            "src.grouping",
            "src.manual_io",
            "src.normalize",
            "src.performance",
            "src.salesforce",
            "src.similarity",
            "src.survivorship",
            # App modules
            "app.components.controls",
            "app.components.export",
            "app.components.group_details",
            "app.components.group_list",
            "app.components.maintenance",
            "app.main",
        ]
    
        for module_name in modules_to_test:
            try:
                module = __import__(module_name, fromlist=[""])
                assert module is not None
            except ImportError as e:
>               pytest.fail(f"Failed to import {module_name}: {e}")
E               Failed: Failed to import src.salesforce: No module named 'src.salesforce'

tests/test_imports.py:52: Failed
____________________________ test_validate_csv_file ____________________________

    def test_validate_csv_file():
        """Test CSV file validation."""
        # Test with valid file
>       assert validate_csv_file("test.csv") is True
E       AssertionError: assert False is True
E        +  where False = validate_csv_file('test.csv')

tests/test_io_utils.py:350: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    src.utils.io_utils:io_utils.py:435 CSV validation failed for test.csv: [Errno 2] No such file or directory: 'test.csv'
___________ TestReadOnlySafety.test_no_destructive_functions_in_code ___________

self = <tests.test_readonly_safety.TestReadOnlySafety object at 0x11ace0680>

    def test_no_destructive_functions_in_code(self) -> None:
        """Test that no destructive functions are called in the codebase."""
        destructive_functions = find_destructive_functions()
    
        # Filter out legitimate uses (like in tests or cleanup tools)
        legitimate_uses = set()
        for func in destructive_functions:
            file_path = func.split(":")[0]
            if any(legit in file_path for legit in ["test_", "cleanup_", "tools/"]):
                legitimate_uses.add(func)
    
        # Remove legitimate uses from the set
        problematic_functions = destructive_functions - legitimate_uses
    
>       assert len(problematic_functions) == 0, (
            f"Found potentially destructive functions in production code:\n"
            f"{chr(10).join(sorted(problematic_functions))}\n"
            f"All destructive operations must be gated behind Phase-1 fuses."
        )
E       AssertionError: Found potentially destructive functions in production code:
E         app/components/maintenance.py:213:preview_delete_runs
E         app/components/maintenance.py:233:delete_runs
E         scripts/run_modes_benchmark.py:347:shutil.rmtree
E         All destructive operations must be gated behind Phase-1 fuses.
E       assert 3 == 0
E        +  where 3 = len({'app/components/maintenance.py:213:preview_delete_runs', 'app/components/maintenance.py:233:delete_runs', 'scripts/run_modes_benchmark.py:347:shutil.rmtree'})

tests/test_readonly_safety.py:231: AssertionError
__________ TestReadOnlySafety.test_maintenance_ui_shows_readonly_copy __________

self = <tests.test_readonly_safety.TestReadOnlySafety object at 0x11ace09b0>

    def test_maintenance_ui_shows_readonly_copy(self) -> None:
        """Test that maintenance UI shows the correct read-only message."""
>       assert (
            check_maintenance_ui_copy()
        ), "Maintenance UI must show: 'Run deletion functionality will be implemented in a future phase.'"
E       AssertionError: Maintenance UI must show: 'Run deletion functionality will be implemented in a future phase.'
E       assert False
E        +  where False = check_maintenance_ui_copy()

tests/test_readonly_safety.py:259: AssertionError
___________ TestReadOnlySafety.test_maintenance_rendered_in_sidebar ____________

self = <tests.test_readonly_safety.TestReadOnlySafety object at 0x11ace0b30>

    def test_maintenance_rendered_in_sidebar(self) -> None:
        """Test that maintenance is rendered in sidebar context."""
>       assert (
            check_sidebar_placement()
        ), "Maintenance component must be rendered in sidebar using st.sidebar.subheader"
E       AssertionError: Maintenance component must be rendered in sidebar using st.sidebar.subheader
E       assert False
E        +  where False = check_sidebar_placement()

tests/test_readonly_safety.py:265: AssertionError
______________________ test_header_list_raises_typeerror _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x11c2756d0>

    def test_header_list_raises_typeerror(monkeypatch: pytest.MonkeyPatch) -> None:
        def fake_parallel(*args, **kwargs):
            return ["id_a", "id_b", "score", "ratio_name", "ratio_set"]
    
>       monkeypatch.setattr(sim, "_compute_similarity_scores_parallel", fake_parallel)
E       AttributeError: <module 'src.similarity' from '/Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/similarity/__init__.py'> has no attribute '_compute_similarity_scores_parallel'

tests/test_similarity_header_list_regression.py:11: AttributeError
=============================== warnings summary ===============================
tests/perf/test_groups_bench.py:218
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:218: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:245
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:245: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:273
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:273: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:305
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:305: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:337
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:337: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:365
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:365: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:397
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:397: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:430
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:430: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:467
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:467: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:503
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:503: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/test_cache_keys.py:11
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_keys.py:11: PytestUnknownMarkWarning: Unknown pytest.mark.duckdb - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytestmark = [pytest.mark.duckdb, pytest.mark.pyarrow]

tests/test_cache_keys.py:11
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_keys.py:11: PytestUnknownMarkWarning: Unknown pytest.mark.pyarrow - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytestmark = [pytest.mark.duckdb, pytest.mark.pyarrow]

tests/test_duckdb_query_params.py:13
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_duckdb_query_params.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.duckdb - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytestmark = [pytest.mark.duckdb]

tests/test_disposition_vectorized_phase1353.py::test_vectorized_vs_legacy_identical_output
tests/test_disposition_vectorized_phase1353.py::test_disposition_classification_correctness
tests/test_disposition_vectorized_phase1353.py::test_feature_flag_rollback
tests/test_disposition_vectorized_phase1353.py::test_performance_improvement
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/disposition.py:570: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.
    suspicious_singleton_mask = name_series.str.contains(

tests/test_disposition_vectorized_phase1353.py::test_vectorized_vs_legacy_identical_output
tests/test_disposition_vectorized_phase1353.py::test_disposition_classification_correctness
tests/test_disposition_vectorized_phase1353.py::test_feature_flag_rollback
tests/test_disposition_vectorized_phase1353.py::test_performance_improvement
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/disposition.py:861: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.
    suspicious_singleton_mask = name_series.str.contains(

tests/test_id_utils.py::TestNormalizeSfidSeries::test_normalize_sfid_series_handles_nan
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/utils/id_utils.py:110: FutureWarning: Operation between non boolean Series with different indexes will no longer return a boolean result in a future version. Cast both Series to object type to maintain the prior behavior.
    out.loc[non_empty & is15] = s_filtered[is15].map(sfid15_to_18)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /Users/joe.j/Documents/dev/salesforce/apps/company_junction/artifacts/qa/junit_final.xml -
================================ tests coverage ================================
_______________ coverage: platform darwin, python 3.12.2-final-0 _______________

Name                                 Stmts   Miss Branch BrPart  Cover   Missing
--------------------------------------------------------------------------------
src/alias_matching.py                  293    115    138     27  56.8%   46, 48->44, 82-83, 84->77, 130-134, 138-139, 141, 145, 154->152, 163, 168-170, 176, 179-268, 387, 396, 407, 442-450, 477-478, 486->484, 505-509, 563, 624-628, 633-634, 637, 649, 673->643, 681, 698-701, 720-744, 756-785, 798-804
src/cleaning.py                        618    572    160      3   6.6%   110-117, 138-189, 212-282, 299, 325-333, 347-392, 434-1642, 1647-1767, 1771
src/disposition.py                     317     36    126     23  86.7%   86, 91, 103, 125-127, 146->144, 150-152, 171->178, 214, 276, 325, 333->338, 373, 392, 569->575, 604->602, 631, 658, 672-676, 682, 714-715, 728-734, 761, 766-767, 771, 778, 818->816, 860->867
src/dtypes_map.py                       14      0      8      0 100.0%
src/grouping.py                        308     90    108     30  67.8%   166-174, 189, 208, 213, 217, 220-225, 246-247, 261-262, 278-281, 291->314, 308-312, 316-323, 329-351, 353->394, 366, 368, 370, 377-378, 382-383, 400-423, 429-432, 450-452, 484, 508, 520-524, 564-569, 582, 611, 614->618, 654, 656, 678-681, 698, 726, 729->732
src/manual_io.py                        83     46     18      3  45.5%   18-20, 34-54, 77-79, 96-101, 129->128, 131->128, 136-145, 162-168, 191-202, 219-225, 242-244
src/normalize.py                       205     35     80     12  81.4%   243-244, 292->291, 295->291, 305->302, 338, 345, 359-364, 378, 421, 425-443, 453->456, 458-462, 480-481, 517-519, 600
src/performance.py                      78     56     16      0  23.4%   21-22, 31-36, 40-46, 50-54, 58-59, 63-72, 93-96, 149-155, 168-191, 208-221
src/similarity/blocking.py             184     53     82     14  65.8%   75->74, 77, 81->85, 102->132, 106->103, 137, 149, 176-189, 228->233, 257, 275-297, 317->316, 334-345, 360-364, 377-407, 413-425
src/similarity/diagnostics.py           51      7     16      3  85.1%   24->31, 32-40, 100
src/similarity/scoring.py              101      6     24      1  94.4%   149-173
src/similarity/types.py                 10      0      0      0 100.0%
src/survivorship.py                    236    137     74     10  40.3%   17-18, 47-55, 62-63, 80-88, 125, 128->132, 144->149, 152->161, 162-169, 207-265, 289, 376->371, 402-438, 455-479, 500-586, 605-672, 683-684, 697-703
src/utils/artifact_management.py        15      5      4      1  57.9%   30-36
src/utils/cache_keys.py                 69      8     16      2  88.2%   44, 46, 102-103, 142-143, 187-188
src/utils/cache_utils.py               342    306    128      0   7.7%   36-40, 49-65, 70-72, 77-87, 92-101, 106-112, 122-146, 151-158, 163-197, 202-224, 229-262, 267-290, 303-356, 369-459, 469-491, 496-519, 524-538, 543-548, 561-599, 612-634, 639-650
src/utils/cli_builder.py                86      8     64      2  88.0%   90-98, 173-174
src/utils/dtypes.py                     81     14     26      6  81.3%   17-18, 51-54, 65-69, 129, 169->173, 180->186, 204, 211, 226-227
src/utils/duckdb_group_stats.py        132     20     24      5  84.0%   89-91, 113->115, 115->118, 191->225, 221-222, 272-274, 320-378, 382->exit
src/utils/duckdb_utils.py               29     15     10      1  43.6%   14-22, 27-30, 39-42, 47->46
src/utils/exact_equals.py              105     32     38      4  67.8%   84-85, 124, 172->170, 222-267, 292
src/utils/filtering.py                  95     27     34      6  69.8%   73-91, 119, 144, 150->155, 157-161, 164, 180-185, 223-228, 234-239
src/utils/fragment_utils.py             12      0      2      0 100.0%
src/utils/group_details.py             231     89     56     15  56.8%   90->89, 107-110, 120-122, 124-129, 151-154, 170->176, 185-188, 205-328, 342, 351, 364-369, 435, 454, 463-464, 474, 481, 492, 501-506
src/utils/group_pagination.py          478    327     68      8  31.0%   102-110, 120-123, 130-131, 175-178, 197-200, 203-214, 217-229, 252-255, 263-274, 285-288, 324-487, 511-725, 764, 771-772, 781, 820, 905-910, 924-956, 973-1031, 1048-1106
src/utils/group_stats.py                46     30     10      0  28.6%   39-42, 56-157
src/utils/hash_utils.py                 47     29     18      0  27.7%   34-65, 79-80, 103, 121, 179-181
src/utils/id_utils.py                   50      1     26      1  97.4%   96
src/utils/io_utils.py                  145     29     44      5  78.8%   101-103, 127, 140-145, 158-162, 184-191, 249-251, 314->318, 338-340, 359-360, 374->388, 381->388, 383->388, 407
src/utils/logging_utils.py               5      1      0      0  80.0%   13
src/utils/metrics.py                    56     39     20      3  26.3%   14-75, 93-96, 101-107, 115, 120-123, 128-131, 136-139, 144-147
src/utils/mini_dag.py                  235     56     88     17  73.7%   53->exit, 83-93, 106, 110, 133-140, 150-153, 155, 158, 180-183, 194-195, 209-221, 247, 262-265, 269, 287-291, 324-327, 331->330, 339-341, 404, 409, 446-448, 454-455, 471-472
src/utils/opt_deps.py                   21      0      2      0 100.0%
src/utils/parallel_protocols.py          8      0      0      0 100.0%
src/utils/parallel_utils.py            247     90    100      7  62.8%   35-38, 47-67, 72-77, 85-87, 134->141, 136->138, 138->141, 180-214, 223-224, 362, 396-424, 445->448, 516-529, 552-560, 632-638
src/utils/parity_validator.py          126     33     48     11  69.0%   67, 69, 72-74, 82-84, 113, 175, 196-233, 264, 284-285, 314-316, 333->344, 351-352, 370-372
src/utils/parquet_size_reporter.py     149    132     44      0   8.8%   30, 42-102, 106-140, 144-154, 158-198, 202-221, 240-288, 298-322, 327, 341-396
src/utils/path_utils.py                 99     47     42      7  48.9%   16, 26, 36-37, 84, 103, 105, 127, 129, 142-163, 213-222, 227-240
src/utils/perf_utils.py                 94     56     34      4  32.8%   28-29, 62, 65, 67->exit, 74-80, 105-148, 170-199, 212-228, 270-274, 284
src/utils/pipeline_constants.py         10      0      0      0 100.0%
src/utils/progress.py                   64     21     20      8  63.1%   35-40, 44, 47, 54->58, 60-65, 71, 73-75, 81-84
src/utils/resource_monitor.py          103     13     22      6  84.8%   19-21, 31->48, 58, 129-131, 164-166, 187-188, 200->206, 206->211, 227->239, 236
src/utils/run_management.py            112    112     44      0   0.0%   6-302
src/utils/schema_utils.py              229     25     88     13  86.8%   147-153, 200->210, 203->202, 205->210, 270, 332->343, 343->350, 350->356, 352->356, 396->389, 402, 420->411, 427, 469-470, 493-502, 528, 541-566
src/utils/settings.py                   35      1     12      1  95.7%   62
src/utils/simple_state.py               56     56      2      0   0.0%   7-209
src/utils/sort_utils.py                 28      0     14      0 100.0%
src/utils/sql_utils.py                   8      0      2      0 100.0%
src/utils/state_utils.py                81      1     18      5  93.9%   173->175, 175->177, 177->181, 181->exit, 184
src/utils/ui_session.py                 30     12      4      1  55.9%   22-29, 39-41, 50-51
src/utils/union_find.py                 57     57     12      0   0.0%   7-174
src/utils/validation_utils.py           14      8      0      0  42.9%   15, 20-21, 26, 31-34
--------------------------------------------------------------------------------
TOTAL                                 6328   2853   2034    265  53.4%
Coverage XML written to file artifacts/qa/coverage_final.xml
============================= slowest 20 durations =============================
0.50s call     tests/test_readonly_safety.py::TestReadOnlySafety::test_no_destructive_functions_in_code
0.46s call     tests/test_readonly_safety.py::TestReadOnlySafety::test_no_direct_run_index_deletions
0.41s call     tests/test_disposition_vectorized_phase1353.py::test_performance_improvement
0.27s call     tests/test_env_clamp.py::test_parallel_map_uses_blas_clamp
0.22s call     tests/test_group_stats_memoization.py::TestGroupStatsMemoization::test_duckdb_memoization_smoke
0.09s call     tests/test_cache_keys.py::test_deterministic_pagination
0.08s call     tests/test_parallel_execution.py::test_worker_count_variations
0.06s call     tests/test_parallel_execution.py::test_parallel_executor_initialization
0.06s call     tests/test_parallel_execution.py::test_deterministic_execution
0.06s call     tests/test_io_utils.py::TestSettingsCaching::test_reload_settings
0.06s call     tests/test_parallel_execution.py::test_backend_comparison
0.05s call     tests/lints/test_no_ui_helpers_import.py::test_no_ui_helpers_import
0.04s call     tests/test_cleaning.py::TestCleaning::test_enhanced_filtering_removes_problematic_records
0.03s call     tests/test_env_clamp.py::test_parallel_map_sequential_fallback
0.03s call     tests/test_parallel_execution.py::test_should_use_parallel
0.03s call     tests/test_io_utils.py::TestHelperFunctions::test_cache_clearing
0.03s call     tests/test_scoring_bounds.py::TestScoringBounds::test_score_clamp_upper_bound
0.03s call     tests/test_no_hardcoding.py::test_config_structure
0.03s call     tests/test_parallel_execution.py::test_small_input_guard
0.03s call     tests/test_io_utils.py::TestSettingsCaching::test_cache_clearing
=========================== short test summary info ============================
SKIPPED [2] tests/test_alias_validation.py:182: TODO: Phase 1.26.1 - Temporarily skipped for QA efficiency, will re-enable after Phase 1.25.1
SKIPPED [1] tests/test_cache_utils.py:176: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:198: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:220: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:240: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:264: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:287: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:308: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:329: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:342: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_cache_utils.py:369: TODO: Phase 1.26.1 - Update cache utils tests for new path utilities
SKIPPED [1] tests/test_schema_casing.py:47: Could not test schema casing: [Errno 2] No such file or directory: 'data/processed/latest/group_stats.parquet'
SKIPPED [1] tests/test_schema_casing.py:66: Could not test review_ready schema casing: [Errno 2] No such file or directory: 'data/processed/latest/review_ready.parquet'
SKIPPED [1] tests/test_schema_casing.py:81: Could not test group_details schema casing: [Errno 2] No such file or directory: 'data/processed/latest/group_details.parquet'
ERROR tests/test_cache_utils.py::test_latest_pointer_operations
FAILED tests/test_alias_matching_parallelism.py::TestAliasMatchingParallelism::test_parallel_executor_integration
FAILED tests/test_cleaning.py::TestCleaning::test_validate_required_columns
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_basic
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_with_parallelism
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_no_parallel
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_with_run_control
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_with_extra_args
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_custom_outdir
FAILED tests/test_details_fast_path.py::TestGroupDetailsDuckDB::test_get_group_details_duckdb_file_not_found
FAILED tests/test_details_fast_path.py::TestGroupsRouting::test_groups_use_duckdb_when_stats_parquet_exists
FAILED tests/test_details_fast_path.py::TestGroupsRouting::test_groups_fallback_to_pyarrow_when_duckdb_unavailable
FAILED tests/test_disposition.py::TestDisposition::test_manual_override_application
FAILED tests/test_disposition.py::TestDisposition::test_multiple_names_verification
FAILED tests/test_duckdb_group_stats_phase1354.py::test_duckdb_memoization - ...
FAILED tests/test_duckdb_group_stats_phase1354.py::test_duckdb_parquet_write
FAILED tests/test_duckdb_group_stats_phase1354.py::test_parquet_size_reporter
FAILED tests/test_duckdb_group_stats_phase1354.py::test_performance_improvement
FAILED tests/test_duckdb_query_params.py::test_no_f_string_sql_queries - Fail...
FAILED tests/test_imports.py::test_absolute_imports - Failed: Failed to impor...
FAILED tests/test_io_utils.py::test_validate_csv_file - AssertionError: asser...
FAILED tests/test_readonly_safety.py::TestReadOnlySafety::test_no_destructive_functions_in_code
FAILED tests/test_readonly_safety.py::TestReadOnlySafety::test_maintenance_ui_shows_readonly_copy
FAILED tests/test_readonly_safety.py::TestReadOnlySafety::test_maintenance_rendered_in_sidebar
FAILED tests/test_similarity_header_list_regression.py::test_header_list_raises_typeerror
