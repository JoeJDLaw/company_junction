============================= test session starts ==============================
platform darwin -- Python 3.12.2, pytest-8.4.1, pluggy-1.6.0
rootdir: /Users/joe.j/Documents/dev/salesforce/apps/company_junction
configfile: pytest.ini
plugins: cov-6.2.1, mock-3.14.1
collected 809 items

tests/contracts/test_parquet_contracts.py FFFFF.F.......FF.F             [  2%]
tests/lints/test_no_schema_fragile_hardcoding.py FFF.                    [  2%]
tests/lints/test_no_ui_helpers_import.py ....                            [  3%]
tests/perf/test_groups_bench.py EEEEEEEEEE                               [  4%]
tests/test_alias_equivalence.py ....                                     [  4%]
tests/test_alias_matching.py .                                           [  5%]
tests/test_alias_matching_parallelism.py F........                       [  6%]
tests/test_alias_progress_logger.py ....                                 [  6%]
tests/test_alias_validation.py ...ss                                     [  7%]
tests/test_brand_suggestions.py .....                                    [  7%]
tests/test_cache_keys.py ........................                        [ 10%]
tests/test_cache_utils.py Essssssssss                                    [ 12%]
tests/test_changelog_dates.py ...                                        [ 12%]
tests/test_cleaning.py ....F                                             [ 13%]
tests/test_cleanup.py .....................                              [ 15%]
tests/test_cleanup_empty_state.py ..............                         [ 17%]
tests/test_cleanup_keep_at_least_guard.py ...                            [ 17%]
tests/test_cleanup_reconcile.py ...........                              [ 19%]
tests/test_cli_builder.py ...............FFFFFF...                       [ 22%]
tests/test_cli_resume_force.py ..                                        [ 22%]
tests/test_details_fast_path.py ........F.FF                             [ 23%]
tests/test_disposition.py ........FF.....                                [ 25%]
tests/test_disposition_vectorized_phase1353.py ......                    [ 26%]
tests/test_dtypes.py ....................                                [ 29%]
tests/test_duckdb_group_stats_phase1354.py ..FF.F.F                      [ 30%]
tests/test_duckdb_query_params.py F..                                    [ 30%]
tests/test_e2e_run_id_and_determinism.py ssss                            [ 30%]
tests/test_env_clamp.py .....                                            [ 31%]
tests/test_exact_equals_phase1352.py .....                               [ 32%]
tests/test_fragment_utils.py ......                                      [ 32%]
tests/test_group_artifacts_scoped.py ....                                [ 33%]
tests/test_group_stats_memoization.py ....                               [ 33%]
tests/test_grouping.py ...............                                   [ 35%]
tests/test_groups_pagination.py .............F.F..FF...                  [ 38%]
tests/test_id_utils.py ......................                            [ 41%]
tests/test_import_audit.py F.........                                    [ 42%]
tests/test_imports.py F.F                                                [ 42%]
tests/test_interrupt_resume.py ...F.....                                 [ 44%]
tests/test_io_utils.py ..................F...                            [ 46%]
tests/test_mini_dag.py ..............                                    [ 48%]
tests/test_mini_dag_resume.py ...................                        [ 50%]
tests/test_mini_dag_resume_contract.py .....                             [ 51%]
tests/test_mini_dag_state_transitions.py ....                            [ 51%]
tests/test_no_hardcoding.py ....                                         [ 52%]
tests/test_normalize.py ..............                                   [ 54%]
tests/test_parallel_execution.py ...........                             [ 55%]
tests/test_parallel_utils.py ...................                         [ 57%]
tests/test_perf_utils.py FF..                                            [ 58%]
tests/test_readonly_safety.py F.FF..                                     [ 59%]
tests/test_resource_monitor.py ..............                            [ 60%]
tests/test_run_maintenance_ui.py ....                                    [ 61%]
tests/test_schema_apply.py ....                                          [ 61%]
tests/test_schema_casing.py sss.                                         [ 62%]
tests/test_schema_resolver.py .......................                    [ 65%]
tests/test_scoring_bounds.py ..............                              [ 66%]
tests/test_scoring_bulk_gate.py .....                                    [ 67%]
tests/test_scoring_bulk_parity.py .........                              [ 68%]
tests/test_scoring_components.py ..........                              [ 69%]
tests/test_scoring_config_defaults.py .............                      [ 71%]
tests/test_scoring_config_toggles.py .............                       [ 73%]
tests/test_scoring_contracts.py ...........                              [ 74%]
tests/test_scoring_degenerate.py ..............                          [ 76%]
tests/test_scoring_enhanced_fallback.py ............                     [ 77%]
tests/test_scoring_logging.py .....                                      [ 78%]
tests/test_scoring_logging_bulk_gate.py ............                     [ 79%]
tests/test_scoring_output_persistence.py ......                          [ 80%]
tests/test_scoring_penalties.py ..............                           [ 82%]
tests/test_scoring_persistence.py ............                           [ 83%]
tests/test_scoring_robustness.py .................                       [ 85%]
tests/test_scoring_threshold_sort.py ..........                          [ 87%]
tests/test_similarity.py ....                                            [ 87%]
tests/test_similarity_extend_regression.py .                             [ 87%]
tests/test_similarity_fix.py .......                                     [ 88%]
tests/test_similarity_fixes.py ......                                    [ 89%]
tests/test_similarity_header_list_regression.py F                        [ 89%]
tests/test_similarity_improvements.py ......                             [ 90%]
tests/test_similarity_refactor.py ......                                 [ 90%]
tests/test_similarity_scores_columns.py .                                [ 90%]
tests/test_similarity_shape_guard.py .                                   [ 91%]
tests/test_sort_context_consistency.py ................                  [ 93%]
tests/test_sort_spec.py ............                                     [ 94%]
tests/test_sort_utils.py ....................                            [ 97%]
tests/test_state_utils.py ................                               [ 99%]
tests/test_survivorship_equivalence.py ........                          [100%]

==================================== ERRORS ====================================
___ ERROR at setup of TestGroupsPageBenchmarks.test_groups_page_10k_pyarrow ____
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 214
      @pytest.mark.performance
      def test_groups_page_10k_pyarrow(self, benchmark, synthetic_10k_paths, monkeypatch):
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:214
____ ERROR at setup of TestGroupsPageBenchmarks.test_groups_page_10k_duckdb ____
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 241
      @pytest.mark.performance
      def test_groups_page_10k_duckdb(self, benchmark, synthetic_10k_paths, monkeypatch):
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:241
___ ERROR at setup of TestGroupsPageBenchmarks.test_groups_page_100k_pyarrow ___
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 269
      @pytest.mark.performance
      def test_groups_page_100k_pyarrow(
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:269
___ ERROR at setup of TestGroupsPageBenchmarks.test_groups_page_100k_duckdb ____
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 301
      @pytest.mark.performance
      def test_groups_page_100k_duckdb(
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:301
____ ERROR at setup of TestGroupsPageBenchmarks.test_groups_page_1m_pyarrow ____
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 333
      @pytest.mark.performance
      @pytest.mark.slow
      def test_groups_page_1m_pyarrow(self, benchmark, synthetic_1m_paths, monkeypatch):
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:333
____ ERROR at setup of TestGroupsPageBenchmarks.test_groups_page_1m_duckdb _____
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 361
      @pytest.mark.performance
      @pytest.mark.slow
      def test_groups_page_1m_duckdb(self, benchmark, synthetic_1m_paths, monkeypatch):
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:361
_ ERROR at setup of TestGroupDetailsBenchmarks.test_group_details_10k_pyarrow __
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 393
      @pytest.mark.performance
      def test_group_details_10k_pyarrow(
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:393
__ ERROR at setup of TestGroupDetailsBenchmarks.test_group_details_10k_duckdb __
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 426
      @pytest.mark.performance
      def test_group_details_10k_duckdb(
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:426
__________ ERROR at setup of TestSortVariants.test_sort_variants_10k ___________
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 463
      @pytest.mark.performance
      def test_sort_variants_10k(self, benchmark, synthetic_10k_paths, monkeypatch):
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:463
________ ERROR at setup of TestFilterVariants.test_filter_variants_10k _________
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py, line 499
      @pytest.mark.performance
      def test_filter_variants_10k(self, benchmark, synthetic_10k_paths, monkeypatch):
E       fixture 'benchmark' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, session_mocker, synthetic_100k_data, synthetic_100k_paths, synthetic_10k_data, synthetic_10k_paths, synthetic_1m_data, synthetic_1m_paths, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:499
_______________ ERROR at setup of test_latest_pointer_operations _______________
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_utils.py, line 154
  def test_latest_pointer_operations(sample_runs, cache_utils_workspace):
file /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_utils.py, line 22
  @pytest.fixture
  def sample_runs(cache_utils_workspace):
E       fixture 'cache_utils_workspace' not found
>       available fixtures: cache, capfd, capfdbinary, caplog, capsys, capsysbinary, capteesys, class_mocker, cov, doctest_namespace, failed_runs, mocker, module_mocker, monkeypatch, no_cover, package_mocker, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, sample_runs, session_mocker, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory
>       use 'pytest --fixtures [testpath]' for help on them.

/Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_utils.py:22
=================================== FAILURES ===================================
_______________ TestParquetContracts.test_required_columns_exist _______________

self = <tests.contracts.test_parquet_contracts.TestParquetContracts object at 0x107d4fa40>
artifact_paths = {'group_details_parquet': '/path/to/group_details.parquet', 'group_stats_parquet': '/path/to/group_stats.parquet', 'review_ready_parquet': '/path/to/review_ready.parquet'}

    def test_required_columns_exist(self, artifact_paths):
        """Test that all required columns exist in parquet files."""
        for parquet_type, required in REQUIRED_COLUMNS.items():
            if parquet_type in artifact_paths:
                file_path = artifact_paths[parquet_type]
                result = validate_parquet_schema(file_path, parquet_type)
    
                assert result["valid"], (
                    f"{parquet_type} schema validation failed: {result['error']}\n"
                    f"Required: {required}\n"
>                   f"Actual: {result['actual_columns']}\n"
                               ^^^^^^^^^^^^^^^^^^^^^^^^
                    f"Missing: {result['missing_columns']}"
                )
E               KeyError: 'actual_columns'

tests/contracts/test_parquet_contracts.py:145: KeyError
_________________ TestParquetContracts.test_schema_consistency _________________

self = <tests.contracts.test_parquet_contracts.TestParquetContracts object at 0x1063473e0>
artifact_paths = {'group_details_parquet': '/path/to/group_details.parquet', 'group_stats_parquet': '/path/to/group_stats.parquet', 'review_ready_parquet': '/path/to/review_ready.parquet'}

    def test_schema_consistency(self, artifact_paths):
        """Test that schemas are consistent across runs."""
        # This test would compare schemas across multiple runs
        # For now, just ensure we can read the schema
        for parquet_type in REQUIRED_COLUMNS:
            if parquet_type in artifact_paths:
                file_path = artifact_paths[parquet_type]
                result = validate_parquet_schema(file_path, parquet_type)
    
>               assert result["valid"], f"Schema validation failed for {parquet_type}"
E               AssertionError: Schema validation failed for review_ready_parquet
E               assert False

tests/contracts/test_parquet_contracts.py:158: AssertionError
____________________ TestParquetContracts.test_column_types ____________________

self = <tests.contracts.test_parquet_contracts.TestParquetContracts object at 0x119285fa0>
artifact_paths = {'group_details_parquet': '/path/to/group_details.parquet', 'group_stats_parquet': '/path/to/group_stats.parquet', 'review_ready_parquet': '/path/to/review_ready.parquet'}

    def test_column_types(self, artifact_paths):
        """Test that column types are as expected."""
        expected_types = {
            "group_id": "string",
            "account_name": "string",
            "is_primary": "bool",
            "disposition": "string",
            "group_size": "int64",
            "max_score": "double",
            "primary_name": "string",
            "weakest_edge_to_primary": "double",
        }
    
        for parquet_type in REQUIRED_COLUMNS:
            if parquet_type in artifact_paths:
                file_path = artifact_paths[parquet_type]
                result = validate_parquet_schema(file_path, parquet_type)
    
>               assert result["valid"], f"Schema validation failed for {parquet_type}"
E               AssertionError: Schema validation failed for review_ready_parquet
E               assert False

tests/contracts/test_parquet_contracts.py:179: AssertionError
_____________ TestParquetContracts.test_no_extra_required_columns ______________

self = <tests.contracts.test_parquet_contracts.TestParquetContracts object at 0x1192b3d70>
artifact_paths = {'group_details_parquet': '/path/to/group_details.parquet', 'group_stats_parquet': '/path/to/group_stats.parquet', 'review_ready_parquet': '/path/to/review_ready.parquet'}

    def test_no_extra_required_columns(self, artifact_paths):
        """Test that we don't have unexpected required columns."""
        # This ensures our contract is not too strict
        for parquet_type in REQUIRED_COLUMNS:
            if parquet_type in artifact_paths:
                file_path = artifact_paths[parquet_type]
                result = validate_parquet_schema(file_path, parquet_type)
    
>               assert result["valid"], f"Schema validation failed for {parquet_type}"
E               AssertionError: Schema validation failed for review_ready_parquet
E               assert False

tests/contracts/test_parquet_contracts.py:211: AssertionError
_________________ TestParquetContracts.test_contract_evolution _________________

self = <tests.contracts.test_parquet_contracts.TestParquetContracts object at 0x1192b3d40>
artifact_paths = {'group_details_parquet': '/path/to/group_details.parquet', 'group_stats_parquet': '/path/to/group_stats.parquet', 'review_ready_parquet': '/path/to/review_ready.parquet'}

    def test_contract_evolution(self, artifact_paths):
        """Test that contracts can evolve safely."""
        # This test documents how to safely evolve contracts
        for parquet_type in REQUIRED_COLUMNS:
            if parquet_type in artifact_paths:
                file_path = artifact_paths[parquet_type]
                result = validate_parquet_schema(file_path, parquet_type)
    
>               assert result["valid"], f"Schema validation failed for {parquet_type}"
E               AssertionError: Schema validation failed for review_ready_parquet
E               assert False

tests/contracts/test_parquet_contracts.py:230: AssertionError
______ TestHardcodedColumnAssumptions.test_detect_schema_constants_usage _______

self = <tests.contracts.test_parquet_contracts.TestHardcodedColumnAssumptions object at 0x1192d0bc0>

    def test_detect_schema_constants_usage(self):
        """Test that schema constants are used consistently."""
        # Check that DETAILS_COLUMNS and similar constants are defined and used
        src_dir = Path(__file__).parent.parent.parent / "src"
        utils_dir = src_dir / "utils"
    
        schema_files = []
        for py_file in utils_dir.rglob("*.py"):
            try:
                with open(py_file) as f:
                    content = f.read()
    
                # Look for column constant definitions
                if "DETAILS_COLUMNS" in content or "GROUP_STATS_COLUMNS" in content:
                    schema_files.append(py_file)
    
            except Exception:
                continue
    
        # Ensure we have schema constant definitions
        assert len(schema_files) > 0, "No schema constant definitions found in utils/"
    
        # Check that constants are used consistently
        for schema_file in schema_files:
            try:
                with open(schema_file) as f:
                    content = f.read()
    
                # Look for hardcoded column lists that should use constants
                if (
                    "[" in content
                    and "GROUP_ID" in content
                    and "ACCOUNT_NAME" in content
                ):
                    # This is a potential hardcoded list - check if it's in a constant definition
                    lines = content.split("\n")
                    for i, line in enumerate(lines):
                        if (
                            "[" in line
                            and "GROUP_ID" in line
                            and "ACCOUNT_NAME" in line
                        ):
                            # Check if this line defines a constant
                            if "=" in line and (
                                "DETAILS_COLUMNS" in line
                                or "GROUP_STATS_COLUMNS" in line
                            ):
                                continue  # This is a constant definition, which is OK
>                           pytest.fail(
                                f"Potential hardcoded column list in {schema_file}:{i+1}\n"
                                f"Line: {line.strip()}\n"
                                f"Consider using a schema constant instead.",
                            )
E                           Failed: Potential hardcoded column list in /Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/utils/group_details.py:98
E                           Line: return [GROUP_ID, ACCOUNT_NAME, DISPOSITION]
E                           Consider using a schema constant instead.

tests/contracts/test_parquet_contracts.py:339: Failed
_____ TestLegacyColumnHandling.test_duckdb_handles_present_legacy_columns ______

self = <tests.contracts.test_parquet_contracts.TestLegacyColumnHandling object at 0x1192d1f70>
sample_data_with_legacy_columns = pyarrow.Table
group_id: string
account_id: string
account_name: string
suffix_class: string
created_date: string
dispo...
disposition: [["Keep","Update","Delete"]]
is_primary: [[true,false,false]]
weakest_edge_to_primary: [[0.95,0.85,0.75]]

    def test_duckdb_handles_present_legacy_columns(
        self,
        sample_data_with_legacy_columns,
    ):
        """Test that DuckDB backend works with present legacy columns."""
        with tempfile.NamedTemporaryFile(suffix=".parquet", delete=False) as tmp_file:
            pq.write_table(sample_data_with_legacy_columns, tmp_file.name)
    
            try:
                import duckdb
    
                conn = duckdb.connect()
    
                # Test filtering by min_edge_strength (should work with legacy columns)
                result = conn.execute(
                    f"""
                    SELECT * FROM read_parquet('{tmp_file.name}')
                    WHERE weakest_edge_to_primary >= 0.8
                """,
                ).fetchall()
>               assert len(result) == 3
E               AssertionError: assert 2 == 3
E                +  where 2 = len([('group1', 'acc1', 'Company A', 'INC', '2023-01-01', 'Keep', ...), ('group2', 'acc2', 'Company B', 'LLC', '2023-01-02', 'Update', ...)])

tests/contracts/test_parquet_contracts.py:522: AssertionError
_____ TestLegacyColumnHandling.test_pyarrow_handles_present_legacy_columns _____

self = <tests.contracts.test_parquet_contracts.TestLegacyColumnHandling object at 0x1192d2150>
sample_data_with_legacy_columns = pyarrow.Table
group_id: string
account_id: string
account_name: string
suffix_class: string
created_date: string
dispo...
disposition: [["Keep","Update","Delete"]]
is_primary: [[true,false,false]]
weakest_edge_to_primary: [[0.95,0.85,0.75]]

    def test_pyarrow_handles_present_legacy_columns(
        self,
        sample_data_with_legacy_columns,
    ):
        """Test that PyArrow backend works with present legacy columns."""
        with tempfile.NamedTemporaryFile(suffix=".parquet", delete=False) as tmp_file:
            pq.write_table(sample_data_with_legacy_columns, tmp_file.name)
    
            try:
                table = pq.read_table(tmp_file.name)
                assert table.num_rows == 3
    
                # Test filtering by min_edge_strength (should work with legacy columns)
                filtered = table.filter(
                    pa.compute.greater_equal(
                        table["weakest_edge_to_primary"],
                        pa.scalar(0.8),
                    ),
                )
>               assert filtered.num_rows == 3
E               assert 2 == 3
E                +  where 2 = pyarrow.Table\ngroup_id: string\naccount_id: string\naccount_name: string\nsuffix_class: string\ncreated_date: string\ndispo...1-01","2023-01-02"]]\ndisposition: [["Keep","Update"]]\nis_primary: [[true,false]]\nweakest_edge_to_primary: [[0.95,0.85]].num_rows

tests/contracts/test_parquet_contracts.py:557: AssertionError
_ TestLegacyColumnHandling.test_conditional_filtering_works_with_present_columns _

self = <tests.contracts.test_parquet_contracts.TestLegacyColumnHandling object at 0x106347710>
sample_data_with_legacy_columns = pyarrow.Table
group_id: string
account_id: string
account_name: string
suffix_class: string
created_date: string
dispo...
disposition: [["Keep","Update","Delete"]]
is_primary: [[true,false,false]]
weakest_edge_to_primary: [[0.95,0.85,0.75]]

    def test_conditional_filtering_works_with_present_columns(
        self,
        sample_data_with_legacy_columns,
    ):
        """Test that conditional filtering works when columns are present."""
        with tempfile.NamedTemporaryFile(suffix=".parquet", delete=False) as tmp_file:
            pq.write_table(sample_data_with_legacy_columns, tmp_file.name)
    
            try:
                # Test the conditional filtering logic
                from src.utils.filtering import apply_filters_pyarrow
    
                # Get available columns
                table = pq.read_table(tmp_file.name)
                available_columns = table.column_names
    
                # Test filtering with min_edge_strength (should work with legacy columns)
                filters = {"min_edge_strength": 0.8}
>               filtered = apply_filters_pyarrow(table, filters, available_columns)
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/contracts/test_parquet_contracts.py:612: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/utils/filtering.py:157: in apply_filters_pyarrow
    es_mask = pc.greater_equal(
/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/site-packages/pyarrow/compute.py:252: in wrapper
    return func.call(args, None, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pyarrow/_compute.pyx:386: in pyarrow._compute.Function.call
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   TypeError: Got unexpected argument type <class 'pyarrow._compute.Expression'> for compute function

pyarrow/_compute.pyx:519: TypeError
_ TestNoSchemaFragileHardcoding.test_no_hardcoded_primary_name_in_details_context _

self = <tests.lints.test_no_schema_fragile_hardcoding.TestNoSchemaFragileHardcoding object at 0x1192d28d0>

    def test_no_hardcoded_primary_name_in_details_context(self):
        """Test that group_details context doesn't use hardcoded primary_name."""
        project_root = Path(__file__).parent.parent.parent
        details_files = [
            project_root / "src" / "utils" / "group_details.py",
            project_root / "app" / "components" / "group_details.py",
        ]
    
        violations = []
    
        for file_path in details_files:
            if not file_path.exists():
                continue
    
            references = self.find_fragile_references(file_path)
            for line_num, line_content in references:
                # Check for hardcoded primary_name usage that should use context-aware functions
                if (
                    "primary_name" in line_content.lower()
                    and "context" not in line_content
                ):
                    violations.append(
                        f"{file_path.relative_to(project_root)}:{line_num} - {line_content}",
                    )
    
        if violations:
>           pytest.fail(
                f"Found {len(violations)} hardcoded primary_name references in details context:\n"
                + "\n".join(violations)
                + "\n\nUse get_order_by(sort_key, context='group_details') or build_sort_expression(sort_key, context='group_details') instead.",
            )
E           Failed: Found 2 hardcoded primary_name references in details context:
E           app/components/group_details.py:67 - primary_name,
E           app/components/group_details.py:79 - primary_name,
E           
E           Use get_order_by(sort_key, context='group_details') or build_sort_expression(sort_key, context='group_details') instead.

tests/lints/test_no_schema_fragile_hardcoding.py:160: Failed
_ TestNoSchemaFragileHardcoding.test_no_hardcoded_weakest_edge_without_availability_check _

self = <tests.lints.test_no_schema_fragile_hardcoding.TestNoSchemaFragileHardcoding object at 0x1192d2a50>

    def test_no_hardcoded_weakest_edge_without_availability_check(self):
        """Test that WEAKEST_EDGE_TO_PRIMARY is not used without availability checks."""
        project_root = Path(__file__).parent.parent.parent
        search_dirs = ["src/utils", "app/components"]
    
        violations = []
    
        for dir_name in search_dirs:
            dir_path = project_root / dir_name
            if not dir_path.exists():
                continue
    
            for file_path in dir_path.rglob("*.py"):
                if self.should_exclude_file(file_path):
                    continue
    
                references = self.find_fragile_references(file_path)
                for line_num, line_content in references:
                    # Check for WEAKEST_EDGE_TO_PRIMARY usage without availability checks
                    if "WEAKEST_EDGE_TO_PRIMARY" in line_content:
                        if not any(
                            pattern in line_content
                            for pattern in [
                                "available_columns",
                                "in.*available_columns",
                                "is.*None.*or",
                                "context=",
                                "get_order_by",
                                "build_sort_expression",
                                "_build_where_clause",
                                "apply_filters_pyarrow",
                            ]
                        ):
                            violations.append(
                                f"{file_path.relative_to(project_root)}:{line_num} - {line_content}",
                            )
    
        if violations:
>           pytest.fail(
                f"Found {len(violations)} WEAKEST_EDGE_TO_PRIMARY references without availability checks:\n"
                + "\n".join(violations)
                + "\n\nUse conditional filtering with available_columns or context-aware functions.",
            )
E           Failed: Found 10 WEAKEST_EDGE_TO_PRIMARY references without availability checks:
E           src/utils/filtering.py:17 - WEAKEST_EDGE_TO_PRIMARY,
E           src/utils/filtering.py:158 - table[WEAKEST_EDGE_TO_PRIMARY],
E           src/utils/group_details.py:43 - WEAKEST_EDGE_TO_PRIMARY,
E           src/utils/group_details.py:125 - where_sql.append(WEAKEST_EDGE_TO_PRIMARY + " >= ?")
E           src/utils/group_pagination.py:28 - WEAKEST_EDGE_TO_PRIMARY,
E           src/utils/group_pagination.py:603 - WEAKEST_EDGE_TO_PRIMARY,
E           src/utils/group_pagination.py:614 - stats_select += f", MAX({WEAKEST_EDGE_TO_PRIMARY}) AS {MAX_SCORE}"
E           app/components/group_details.py:19 - WEAKEST_EDGE_TO_PRIMARY,
E           app/components/group_details.py:280 - WEAKEST_EDGE_TO_PRIMARY,
E           app/components/group_details.py:330 - WEAKEST_EDGE_TO_PRIMARY: st.column_config.NumberColumn(
E           
E           Use conditional filtering with available_columns or context-aware functions.

tests/lints/test_no_schema_fragile_hardcoding.py:204: Failed
_ TestNoSchemaFragileHardcoding.test_no_hardcoded_is_primary_without_availability_check _

self = <tests.lints.test_no_schema_fragile_hardcoding.TestNoSchemaFragileHardcoding object at 0x1192d2bd0>

    def test_no_hardcoded_is_primary_without_availability_check(self):
        """Test that IS_PRIMARY is not used without availability checks."""
        project_root = Path(__file__).parent.parent.parent
        search_dirs = ["src/utils", "app/components"]
    
        violations = []
    
        for dir_name in search_dirs:
            dir_path = project_root / dir_name
            if not dir_path.exists():
                continue
    
            for file_path in dir_path.rglob("*.py"):
                if self.should_exclude_file(file_path):
                    continue
    
                references = self.find_fragile_references(file_path)
                for line_num, line_content in references:
                    # Check for IS_PRIMARY usage without availability checks
                    if "IS_PRIMARY" in line_content:
                        if not any(
                            pattern in line_content
                            for pattern in [
                                "available_columns",
                                "in.*available_columns",
                                r"in.*group_data\.columns",
                                "context=",
                                "get_order_by",
                                "build_sort_expression",
                                "_build_where_clause",
                                "apply_filters_pyarrow",
                                r"\.get\(",
                                "if.*IS_PRIMARY.*in",
                            ]
                        ):
                            violations.append(
                                f"{file_path.relative_to(project_root)}:{line_num} - {line_content}",
                            )
    
        if violations:
>           pytest.fail(
                f"Found {len(violations)} IS_PRIMARY references without availability checks:\n"
                + "\n".join(violations)
                + "\n\nUse conditional filtering with available_columns or context-aware functions.",
            )
E           Failed: Found 7 IS_PRIMARY references without availability checks:
E           src/utils/group_details.py:41 - IS_PRIMARY,
E           src/utils/group_pagination.py:25 - IS_PRIMARY,
E           src/utils/group_pagination.py:602 - IS_PRIMARY,
E           src/utils/group_pagination.py:620 - primary_name_select = f"any_value({ACCOUNT_NAME}) FILTER (WHERE {IS_PRIMARY}) AS {PRIMARY_NAME}"
E           app/components/group_details.py:17 - IS_PRIMARY,
E           app/components/group_details.py:279 - IS_PRIMARY,
E           app/components/group_details.py:326 - IS_PRIMARY: st.column_config.CheckboxColumn(
E           
E           Use conditional filtering with available_columns or context-aware functions.

tests/lints/test_no_schema_fragile_hardcoding.py:250: Failed
_______ TestAliasMatchingParallelism.test_parallel_executor_integration ________

self = <Mock name='mock.should_use_parallel' id='5048294224'>

    def assert_called_once(self):
        """assert that the mock was called only once.
        """
        if not self.call_count == 1:
            msg = ("Expected '%s' to have been called once. Called %s times.%s"
                   % (self._mock_name or 'mock',
                      self.call_count,
                      self._calls_repr()))
>           raise AssertionError(msg)
E           AssertionError: Expected 'should_use_parallel' to have been called once. Called 2 times.
E           Calls: [call(5), call(5)].

/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/unittest/mock.py:923: AssertionError

During handling of the above exception, another exception occurred:

self = <tests.test_alias_matching_parallelism.TestAliasMatchingParallelism object at 0x129a3bd70>

    def test_parallel_executor_integration(self):
        """Test that ParallelExecutor is properly integrated."""
        # Create a mock ParallelExecutor
        mock_executor = Mock(spec=ParallelExecutor)
        mock_executor.should_use_parallel.side_effect = lambda x: True
        mock_executor.workers = 2
        mock_executor.execute_chunked.return_value = [
            [
                {
                    "record_id": 100,
                    "alias_text": "Acme Corp",
                    "match_record_id": 101,
                    "match_group_id": "G1",
                    "score": 95,
                    "suffix_match": True,
                },
            ],
            [
                {
                    "record_id": 101,
                    "alias_text": "Acme Corp",
                    "match_record_id": 100,
                    "match_group_id": "G1",
                    "score": 95,
                    "suffix_match": True,
                },
            ],
        ]
    
        # Call compute_alias_matches with ParallelExecutor
        result_df, stats = compute_alias_matches(
            self.df_norm,
            self.df_groups,
            self.settings,
            parallel_executor=mock_executor,
        )
    
        # Verify ParallelExecutor was used
>       mock_executor.should_use_parallel.assert_called_once()
E       AssertionError: Expected 'should_use_parallel' to have been called once. Called 2 times.
E       Calls: [call(5), call(5)].
E       
E       pytest introspection follows:
E       
E       Args:
E       assert (5,) == ()
E         
E         Left contains one more item: 5
E         Use -v to get more diff

tests/test_alias_matching_parallelism.py:107: AssertionError
_________________ TestCleaning.test_validate_required_columns __________________

self = <tests.test_cleaning.TestCleaning testMethod=test_validate_required_columns>

    def test_validate_required_columns(self) -> None:
        """Test column validation logic."""
        # Test with all required columns
>       self.assertTrue(validate_required_columns(self.sample_data))
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_cleaning.py:78: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

df =   Account ID    Account Name         Relationship Created Date
0        001       Acme Corp  Other/Miscellaneous   202...   003  Tech Solutions  Other/Miscellaneous   2021-01-03
3        004  Tech Solutions  Other/Miscellaneous   2021-01-04

    def validate_required_columns(df: pd.DataFrame) -> bool:
        """Validate that required columns are present.
    
        Args:
            df: DataFrame to validate
    
        Returns:
            True if validation passes
    
        Raises:
            ValueError: If required columns are missing
    
        """
        # Use canonical column names since DataFrame has been renamed
        # Only check for columns that are actually mapped and renamed
        required_columns = [ACCOUNT_ID, ACCOUNT_NAME, CREATED_DATE]
    
        # Check for Account Name (required)
        if ACCOUNT_NAME not in df.columns:
>           raise ValueError(f"Missing required name column: {ACCOUNT_NAME}")
E           ValueError: Missing required name column: account_name

src/cleaning.py:322: ValueError
_______________ TestBuildCliCommand.test_build_cli_command_basic _______________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x129c8a720>

    def test_build_cli_command_basic(self) -> None:
        """Test basic command building."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...settings.yaml' == 'python src/c...settings.yaml'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml
E         ?        ------------------------
E         + est.csv --config config/settings.yaml

tests/test_cli_builder.py:207: AssertionError
_________ TestBuildCliCommand.test_build_cli_command_with_parallelism __________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x129c88110>

    def test_build_cli_command_with_parallelism(self) -> None:
        """Test command with parallelism flags."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            workers=4,
            parallel_backend="threading",
            chunk_size=1000,
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --workers 4 --parallel-backend threading --chunk-size 1000"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...unk-size 1000' == 'python src/c...unk-size 1000'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --workers 4 --parallel-backend threading --chunk-size 1000
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --workers 4 --parallel-backend threading --chunk-size 1000

tests/test_cli_builder.py:219: AssertionError
____________ TestBuildCliCommand.test_build_cli_command_no_parallel ____________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x129c88290>

    def test_build_cli_command_no_parallel(self) -> None:
        """Test command with no-parallel flag."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            no_parallel=True,
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --no-parallel"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...--no-parallel' == 'python src/c...--no-parallel'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --no-parallel
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --no-parallel

tests/test_cli_builder.py:229: AssertionError
_________ TestBuildCliCommand.test_build_cli_command_with_run_control __________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x129c88410>

    def test_build_cli_command_with_run_control(self) -> None:
        """Test command with run control flags."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            no_resume=True,
            run_id="custom_run_123",
            keep_runs=5,
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --no-resume --run-id custom_run_123 --keep-runs 5"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...--keep-runs 5' == 'python src/c...--keep-runs 5'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --no-resume --run-id custom_run_123 --keep-runs 5
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --no-resume --run-id custom_run_123 --keep-runs 5

tests/test_cli_builder.py:241: AssertionError
__________ TestBuildCliCommand.test_build_cli_command_with_extra_args __________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x129c63d10>

    def test_build_cli_command_with_extra_args(self) -> None:
        """Test command with extra arguments."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            extra_args="--verbose --debug",
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir data/processed --config config/settings.yaml --verbose --debug"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...rbose --debug' == 'python src/c...rbose --debug'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir data/processed --config config/settings.yaml --verbose --debug
E         ?        ------------------------
E         + est.csv --config config/settings.yaml --verbose --debug

tests/test_cli_builder.py:251: AssertionError
___________ TestBuildCliCommand.test_build_cli_command_custom_outdir ___________

self = <tests.test_cli_builder.TestBuildCliCommand object at 0x129c60410>

    def test_build_cli_command_custom_outdir(self) -> None:
        """Test command with custom output directory."""
        cmd = build_cli_command(
            input_file="test.csv",
            config="settings.yaml",
            outdir="custom/output",
        )
        expected = "python src/cleaning.py --input data/raw/test.csv --outdir custom/output --config config/settings.yaml"
>       assert cmd == expected
E       AssertionError: assert 'python src/c...custom/output' == 'python src/c...settings.yaml'
E         
E         Skipping 41 identical leading characters in diff, use -v to show
E         - est.csv --outdir custom/output --config config/settings.yaml
E         + est.csv --config config/settings.yaml --outdir custom/output

tests/test_cli_builder.py:261: AssertionError
_____ TestGroupDetailsDuckDB.test_get_group_details_duckdb_file_not_found ______

self = <tests.test_details_fast_path.TestGroupDetailsDuckDB object at 0x129c88800>
mock_exists = <MagicMock name='exists' id='5065006336'>
mock_get_paths = <MagicMock name='get_artifact_paths' id='5063938016'>

    @patch("src.utils.artifact_management.get_artifact_paths")
    @patch("os.path.exists")
    def test_get_group_details_duckdb_file_not_found(
        self,
        mock_exists: MagicMock,
        mock_get_paths: MagicMock,
    ) -> None:
        """Test DuckDB details loading when file not found."""
        # Mock artifact paths
        mock_get_paths.return_value = {
            "group_details_parquet": "/test/path/group_details.parquet",
        }
        mock_exists.return_value = False
    
        # Test the function should raise FileNotFoundError
        with pytest.raises(FileNotFoundError):
>           get_group_details_duckdb(
                "/test/path/group_details.parquet",
                "group1",
                "account_id",
                1,
                10,
                {},
                {},
            )

tests/test_details_fast_path.py:266: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

parquet_path = '/test/path/group_details.parquet', group_id = 'group1'
order_by = 'account_id', page = 1, page_size = 10, filters = {}, settings = {}

    def _get_group_details_duckdb(
        parquet_path: str,
        group_id: str,
        order_by: str,
        page: int,
        page_size: int,
        filters: Dict[str, Any],
        settings: Dict[str, Any],
    ) -> Tuple[List[Dict[str, Any]], int]:
        """DuckDB backend for group details (fast filtering + pagination)."""
        if DUCKDB is None:
            raise ImportError("DuckDB not available for group details")
    
        duckdb_threads = settings.get("ui", {}).get("duckdb_threads", 4)
        timeout_seconds = settings.get("ui", {}).get("timeout_seconds", 30)
    
        start = time.time()
    
        def check_timeout() -> None:
            if time.time() - start > timeout_seconds:
                raise DetailsFetchTimeout(f"Exceeded {timeout_seconds}s")
    
        # Get available columns dynamically
        available_columns = _get_available_columns(parquet_path)
        dynamic_select = _build_dynamic_select(available_columns)
    
        where_clause, params = _build_where_clause(filters, available_columns)
        # Clamp pagination inputs to avoid negative offsets and cap for performance
        page = max(1, int(page))
        requested_size = int(page_size)
        max_page_size = settings.get("ui", {}).get("max_page_size", 250)
        page_size = max(1, min(requested_size, max_page_size))
        if page_size != requested_size:  # Log when clamping occurs
            logger.info(
                "Page size clamped from %s to %s (max_page_size limit)",
                requested_size,
                max_page_size,
            )
            record_page_size_clamped()
        offset = (page - 1) * page_size
    
        # Build SQL using dynamic column selection
        sql = (
            dynamic_select + "WHERE " + GROUP_ID + " = ? AND " + where_clause + " "
            "ORDER BY "
            + order_by
            + " NULLS LAST, "
            + ACCOUNT_NAME
            + " ASC "  # order_by from get_order_by whitelist, stable tie-breaker, NULLs last
            "LIMIT ? OFFSET ?"
        )
        params_page = [parquet_path, group_id, *params, page_size, offset]
    
        count_sql = (
            "SELECT COUNT(*) FROM read_parquet(?) "
            "WHERE " + GROUP_ID + " = ? AND " + where_clause
        )
        params_count = [parquet_path, group_id, *params]
    
        conn = None
        try:
            conn = DUCKDB.connect(":memory:")
            duckdb_threads = int(duckdb_threads or 4)  # Ensure numeric
            duckdb_threads = min(duckdb_threads, 32)  # Double-enforce caps at call site
            conn.execute("PRAGMA threads=" + str(duckdb_threads))
            check_timeout()
    
>           res = conn.execute(sql, params_page)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E           duckdb.duckdb.IOException: IO Error: No files found that match the pattern "/test/path/group_details.parquet"

src/utils/group_details.py:398: IOException
------------------------------ Captured log call -------------------------------
WARNING  src.utils.group_details:group_details.py:96 Failed to read schema from /test/path/group_details.parquet: [Errno 2] Failed to open local file '/test/path/group_details.parquet'. Detail: [errno 2] No such file or directory
______ TestGroupsRouting.test_groups_use_duckdb_when_stats_parquet_exists ______

self = <tests.test_details_fast_path.TestGroupsRouting object at 0x129c89ee0>
mock_exists = <MagicMock name='exists' id='5064435152'>
mock_get_paths = <MagicMock name='get_artifact_paths' id='5064438896'>

    @patch("src.utils.artifact_management.get_artifact_paths")
    @patch("os.path.exists")
    @patch("src.utils.opt_deps.DUCKDB_AVAILABLE", True)
    def test_groups_use_duckdb_when_stats_parquet_exists(
        self,
        mock_exists: MagicMock,
        mock_get_paths: MagicMock,
    ) -> None:
        """Test that groups page uses DuckDB when group_stats.parquet exists."""
        from src.utils.group_pagination import get_groups_page
    
        # Mock artifact paths with group_stats.parquet
        mock_get_paths.return_value = {
            "group_stats_parquet": "/test/path/group_stats.parquet",
        }
        mock_exists.return_value = True
    
        # Mock the DuckDB function to return test data
>       with patch(
            "src.utils.ui_helpers.get_groups_page_from_stats_duckdb",
        ) as mock_duckdb:

tests/test_details_fast_path.py:331: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/unittest/mock.py:1439: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'src.utils.ui_helpers'

    def resolve_name(name):
        """
        Resolve a name to an object.
    
        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:
    
        W(.W)*
        W(.W)*:(W(.W)*)?
    
        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.
    
        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.
    
        The function will return an object (which might be a module), or raise one
        of the following exceptions:
    
        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)
    
        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'src.utils' has no attribute 'ui_helpers'

/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/pkgutil.py:528: AttributeError
__ TestGroupsRouting.test_groups_fallback_to_pyarrow_when_duckdb_unavailable ___

self = <tests.test_details_fast_path.TestGroupsRouting object at 0x129c8a060>
mock_exists = <MagicMock name='exists' id='5063821696'>
mock_get_paths = <MagicMock name='get_artifact_paths' id='5063826448'>

    @patch("src.utils.artifact_management.get_artifact_paths")
    @patch("os.path.exists")
    @patch("src.utils.opt_deps.DUCKDB_AVAILABLE", False)
    def test_groups_fallback_to_pyarrow_when_duckdb_unavailable(
        self,
        mock_exists: MagicMock,
        mock_get_paths: MagicMock,
    ) -> None:
        """Test that groups page falls back to PyArrow when DuckDB unavailable."""
        from src.utils.group_pagination import get_groups_page
    
        # Mock artifact paths with group_stats.parquet
        mock_get_paths.return_value = {
            "group_stats_parquet": "/test/path/group_stats.parquet",
        }
        mock_exists.return_value = True
    
        # Mock the PyArrow function to return test data
>       with patch("src.utils.ui_helpers.get_groups_page_pyarrow") as mock_pyarrow:

tests/test_details_fast_path.py:362: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/unittest/mock.py:1439: in __enter__
    self.target = self.getter()
                  ^^^^^^^^^^^^^
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'src.utils.ui_helpers'

    def resolve_name(name):
        """
        Resolve a name to an object.
    
        It is expected that `name` will be a string in one of the following
        formats, where W is shorthand for a valid Python identifier and dot stands
        for a literal period in these pseudo-regexes:
    
        W(.W)*
        W(.W)*:(W(.W)*)?
    
        The first form is intended for backward compatibility only. It assumes that
        some part of the dotted name is a package, and the rest is an object
        somewhere within that package, possibly nested inside other objects.
        Because the place where the package stops and the object hierarchy starts
        can't be inferred by inspection, repeated attempts to import must be done
        with this form.
    
        In the second form, the caller makes the division point clear through the
        provision of a single colon: the dotted name to the left of the colon is a
        package to be imported, and the dotted name to the right is the object
        hierarchy within that package. Only one import is needed in this form. If
        it ends with the colon, then a module object is returned.
    
        The function will return an object (which might be a module), or raise one
        of the following exceptions:
    
        ValueError - if `name` isn't in a recognised format
        ImportError - if an import failed when it shouldn't have
        AttributeError - if a failure occurred when traversing the object hierarchy
                         within the imported package to get to the desired object.
        """
        global _NAME_PATTERN
        if _NAME_PATTERN is None:
            # Lazy import to speedup Python startup time
            import re
            dotted_words = r'(?!\d)(\w+)(\.(?!\d)(\w+))*'
            _NAME_PATTERN = re.compile(f'^(?P<pkg>{dotted_words})'
                                       f'(?P<cln>:(?P<obj>{dotted_words})?)?$',
                                       re.UNICODE)
    
        m = _NAME_PATTERN.match(name)
        if not m:
            raise ValueError(f'invalid format: {name!r}')
        gd = m.groupdict()
        if gd.get('cln'):
            # there is a colon - a one-step import is all that's needed
            mod = importlib.import_module(gd['pkg'])
            parts = gd.get('obj')
            parts = parts.split('.') if parts else []
        else:
            # no colon - have to iterate to find the package boundary
            parts = name.split('.')
            modname = parts.pop(0)
            # first part *must* be a module/package.
            mod = importlib.import_module(modname)
            while parts:
                p = parts[0]
                s = f'{modname}.{p}'
                try:
                    mod = importlib.import_module(s)
                    parts.pop(0)
                    modname = s
                except ImportError:
                    break
        # if we reach this point, mod is the module, already imported, and
        # parts is the list of parts in the object hierarchy to be traversed, or
        # an empty list if just the module is wanted.
        result = mod
        for p in parts:
>           result = getattr(result, p)
                     ^^^^^^^^^^^^^^^^^^
E           AttributeError: module 'src.utils' has no attribute 'ui_helpers'

/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/pkgutil.py:528: AttributeError
_______________ TestDisposition.test_manual_override_application _______________

self = <tests.test_disposition.TestDisposition testMethod=test_manual_override_application>

    def test_manual_override_application(self) -> None:
        """Test that manual overrides are applied correctly."""
        import json
        from pathlib import Path
    
        # Create test data
        test_data = pd.DataFrame(
            {
                "group_id": [1, 1],
                "Account Name": ["Acme Corp Inc", "Acme Corp Inc"],
                "is_primary": [True, False],
                "weakest_edge_to_primary": [100, 95],
                "suffix_class": ["INC", "INC"],
                "has_multiple_names": [False, False],
            },
        )
    
        # Create manual override file
        manual_dir = Path("data/manual")
        manual_dir.mkdir(parents=True, exist_ok=True)
    
        override_data = [
            {
                "record_id": "0",  # First record
                "account_id": "001Hs000054S8kI",
                "account_name": "Acme Corp Inc",
                "name_core": "acme corp",
                "override": "Delete",
                "reason": "Test override",
                "ts": "2024-01-01T00:00:00",
            },
        ]
    
        with open(manual_dir / "manual_dispositions.json", "w") as f:
            json.dump(override_data, f)
    
        try:
            # Apply dispositions
            result = apply_dispositions(test_data, self.settings)
    
            # Check that manual override was applied
            self.assertEqual(result.iloc[0]["disposition"], "Delete")
>           self.assertEqual(
                result.iloc[0]["disposition_reason"],
                "manual_override:Delete",
            )
E           AssertionError: 'primary_record' != 'manual_override:Delete'
E           - primary_record
E           + manual_override:Delete

tests/test_disposition.py:301: AssertionError
_______________ TestDisposition.test_multiple_names_verification _______________

self = <tests.test_disposition.TestDisposition testMethod=test_multiple_names_verification>

    def test_multiple_names_verification(self) -> None:
        """Test that records with multiple names are marked as Verify."""
        # Create test data with multiple names
        df_groups = self.df_norm.copy()
        df_groups["group_id"] = [0, 1, 2, 3, 4, 5, 6, 7]
        df_groups["is_primary"] = [True] * 8
        df_groups["weakest_edge_to_primary"] = [0.0] * 8
    
        # Add multiple names flag
        df_groups.loc[0, "has_multiple_names"] = True
        df_groups.loc[1, "has_multiple_names"] = True
    
        df_dispositions = apply_dispositions(df_groups, self.settings)
    
        # Check that multiple names are marked as Verify
        self.assertEqual(df_dispositions.iloc[0]["disposition"], "Verify")
        self.assertEqual(df_dispositions.iloc[1]["disposition"], "Verify")
>       self.assertEqual(
            df_dispositions.iloc[0]["disposition_reason"],
            "multi_name_string_requires_split",
        )
E       AssertionError: 'clean_singleton' != 'multi_name_string_requires_split'
E       - clean_singleton
E       + multi_name_string_requires_split

tests/test_disposition.py:254: AssertionError
___________________________ test_duckdb_memoization ____________________________

    def test_duckdb_memoization() -> None:
        """Test that memoization works correctly."""
        df = create_test_group_data()
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            settings = {
                "engine": {"duckdb": {"threads": 2}},
                "io": {"parquet": {"compression": "zstd"}},
>               "group_stats": {"memoization": {"enable": True}, "cache_dir": temp_dir},
                                                                              ^^^^^^^^
            }
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:154: NameError
__________________________ test_duckdb_parquet_write ___________________________

    def test_duckdb_parquet_write() -> None:
        """Test that DuckDB can write optimized Parquet files."""
        df = create_test_group_data()
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            settings = {
                "engine": {"duckdb": {"threads": 2}},
                "io": {"parquet": {"compression": "zstd"}},
>               "group_stats": {"memoization": {"enable": True}, "cache_dir": temp_dir},
                                                                              ^^^^^^^^
            }
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:200: NameError
__________________________ test_parquet_size_reporter __________________________

    def test_parquet_size_reporter() -> None:
        """Test that parquet size reporter works correctly."""
        df = create_test_group_data()
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            # Create test parquet file
>           test_path = f"{temp_dir}/test.parquet"
                           ^^^^^^^^
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:286: NameError
_________________________ test_performance_improvement _________________________

    def test_performance_improvement() -> None:
        """Test that DuckDB is faster than pandas for group stats."""
        # Create larger test dataset
        n_records = 1000
        n_groups = 100
    
        # Ensure each group has exactly one primary record
        group_primary_map = {}
        is_primary_list = []
    
        for i in range(n_records):
            group_id = f"G{i % n_groups}"
            if group_id not in group_primary_map:
                group_primary_map[group_id] = i
                is_primary_list.append(True)
            else:
                is_primary_list.append(False)
    
        df_large = pd.DataFrame(
            {
                "group_id": [f"G{i % n_groups}" for i in range(n_records)],
                "account_id": [f"A{i}" for i in range(n_records)],
                "account_name": [f"Company {i % n_groups}" for i in range(n_records)],
                "is_primary": is_primary_list,
                "weakest_edge_to_primary": [
                    np.random.uniform(80, 100) for _ in range(n_records)
                ],
                "disposition": [
                    "Keep" if is_primary else "Update" for is_primary in is_primary_list
                ],
            },
        )
    
        with tempfile.TemporaryDirectory() as _temp_dir:
            settings = {
                "engine": {"duckdb": {"threads": 2}},
                "io": {"parquet": {"compression": "zstd"}},
>               "group_stats": {"memoization": {"enable": True}, "cache_dir": temp_dir},
                                                                              ^^^^^^^^
            }
E           NameError: name 'temp_dir' is not defined

tests/test_duckdb_group_stats_phase1354.py:348: NameError
_________________________ test_no_f_string_sql_queries _________________________

    def test_no_f_string_sql_queries():
        """Verify no f-string SQL queries exist in group-related modules.
    
        This test ensures all DuckDB queries use parameterized placeholders
        instead of string interpolation to prevent SQL injection.
        """
        # Files to check for SQL queries
        sql_files = [
            "src/utils/group_pagination.py",
            "src/utils/group_details.py",
            "src/utils/group_stats.py",
            "src/utils/filtering.py",
        ]
    
        violations = []
    
        for file_path in sql_files:
            if Path(file_path).exists():
                f_strings = find_string_interpolations(file_path)
                if f_strings:
                    violations.append(
                        f"{file_path}: {len(f_strings)} f-string SQL queries found",
                    )
    
        if violations:
>           pytest.fail(
                "Found f-string SQL queries. All DuckDB queries must use parameterized placeholders:\n"
                + "\n".join(violations)
                + "\n\nExample of correct usage:\n"
                + "query = 'SELECT * FROM groups WHERE run_id = ?'\n"
                + "result = conn.execute(query, [run_id]).fetchdf()",
            )
E           Failed: Found f-string SQL queries. All DuckDB queries must use parameterized placeholders:
E           src/utils/group_pagination.py: 1 f-string SQL queries found
E           src/utils/group_details.py: 2 f-string SQL queries found
E           
E           Example of correct usage:
E           query = 'SELECT * FROM groups WHERE run_id = ?'
E           result = conn.execute(query, [run_id]).fetchdf()

tests/test_duckdb_query_params.py:170: Failed
________ TestGroupStatsComputation.test_compute_group_stats_no_primary _________

self = <tests.test_groups_pagination.TestGroupStatsComputation object at 0x129c67d40>

    def test_compute_group_stats_no_primary(self) -> None:
        """Test handling when no primary record exists."""
        data: Dict[str, List[Any]] = {
            "group_id": ["group1", "group1"],
            "account_name": ["Company A", "Company B"],
            "is_primary": [False, False],
            "weakest_edge_to_primary": [95.0, 85.0],
        }
    
        table = pa.Table.from_pydict(data)
        df = table.to_pandas()
        stats_table = compute_group_stats(df)
        stats_df = stats_table
    
        group1_stats = stats_df[stats_df["group_id"] == "group1"].iloc[0]
>       assert group1_stats["primary_name"] == "Company A"  # First record
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E       AssertionError: assert None == 'Company A'

tests/test_groups_pagination.py:193: AssertionError
________ TestFilterApplication.test_apply_filters_pyarrow_edge_strength ________

self = <tests.test_groups_pagination.TestFilterApplication object at 0x129c665a0>

    def test_apply_filters_pyarrow_edge_strength(self) -> None:
        """Test edge strength filtering."""
        data: Dict[str, List[Any]] = {
            "group_id": ["group1", "group2", "group3"],
            "weakest_edge_to_primary": [95.0, 85.0, 75.0],
        }
    
        table = pa.Table.from_pydict(data)
        filters = {"min_edge_strength": 80.0}
    
>       filtered_table = apply_filters_pyarrow(table, filters)
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

tests/test_groups_pagination.py:226: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
src/utils/filtering.py:157: in apply_filters_pyarrow
    es_mask = pc.greater_equal(
/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/site-packages/pyarrow/compute.py:252: in wrapper
    return func.call(args, None, memory_pool)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
pyarrow/_compute.pyx:386: in pyarrow._compute.Function.call
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

>   ???
E   TypeError: Got unexpected argument type <class 'pyarrow._compute.Expression'> for compute function

pyarrow/_compute.pyx:519: TypeError
___________ TestPaginationLimits.test_pagination_limits_page_bounds ____________

self = <tests.test_groups_pagination.TestPaginationLimits object at 0x129c670b0>

    def test_pagination_limits_page_bounds(self) -> None:
        """Test pagination with page bounds."""
        # Create test data with known number of groups
        data: Dict[str, List[Any]] = {
            "group_id": [f"group{i}" for i in range(10)],
            "account_name": [f"Company {i}" for i in range(10)],
            "is_primary": [True] + [False] * 9,
            "weakest_edge_to_primary": [95.0] * 10,
        }
    
        with tempfile.TemporaryDirectory() as temp_dir:
            table = pa.Table.from_pydict(data)
            parquet_path = os.path.join(temp_dir, "review_ready.parquet")
            pa.parquet.write_table(table, parquet_path)
    
            # Mock artifact paths
            def mock_get_artifact_paths(run_id: str) -> Dict[str, str]:
                return {"review_ready_parquet": parquet_path}
    
            import src.utils.artifact_management
    
            original_get_artifact_paths = (
                src.utils.artifact_management.get_artifact_paths
            )
            src.utils.artifact_management.get_artifact_paths = mock_get_artifact_paths
    
            try:
                # Test first page
                page_groups, total_count = get_groups_page_pyarrow(
                    "test_run",
                    "Group Size (Desc)",
                    1,
                    5,
                    {},
                )
    
>               assert len(page_groups) == 5
E               assert 0 == 5
E                +  where 0 = len([])

tests/test_groups_pagination.py:329: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.utils.group_pagination:group_pagination.py:320 Parquet file not found: data/processed/test_run/review_ready.parquet
_______ TestSortingStability.test_sorting_stability_group_id_tiebreaker ________

self = <tests.test_groups_pagination.TestSortingStability object at 0x129c67b60>

    def test_sorting_stability_group_id_tiebreaker(self) -> None:
        """Test that group_id tiebreaker ensures stable sorting."""
        # Create test data with same values but different group_ids
        data: Dict[str, List[Any]] = {
            "group_id": [
                "group_b",
                "group_b",
                "group_a",
                "group_a",
                "group_c",
                "group_c",
            ],
            "account_name": [
                "Company B1",
                "Company B2",
                "Company A1",
                "Company A2",
                "Company C1",
                "Company C2",
            ],
            "is_primary": [True, False, True, False, True, False],
            "weakest_edge_to_primary": [90.0, 90.0, 90.0, 90.0, 90.0, 90.0],
        }
    
        with tempfile.TemporaryDirectory() as temp_dir:
            table = pa.Table.from_pydict(data)
            parquet_path = os.path.join(temp_dir, "review_ready.parquet")
            pa.parquet.write_table(table, parquet_path)
    
            # Mock artifact paths
            def mock_get_artifact_paths(run_id: str) -> Dict[str, str]:
                return {"review_ready_parquet": parquet_path}
    
            import src.utils.artifact_management
    
            original_get_artifact_paths = (
                src.utils.artifact_management.get_artifact_paths
            )
            src.utils.artifact_management.get_artifact_paths = mock_get_artifact_paths
    
            try:
                # Test ascending sort - should be stable by group_id
                page_groups, _ = get_groups_page_pyarrow(
                    "test_run",
                    "Group Size (Asc)",
                    1,
                    10,
                    {},
                )
    
                # Should be sorted by group_id (ascending) as tiebreaker
                group_ids = [group["group_id"] for group in page_groups]
>               assert group_ids == ["group_a", "group_b", "group_c"]
E               AssertionError: assert [] == ['group_a', '...b', 'group_c']
E                 
E                 Right contains 3 more items, first extra item: 'group_a'
E                 Use -v to get more diff

tests/test_groups_pagination.py:416: AssertionError
------------------------------ Captured log call -------------------------------
WARNING  src.utils.group_pagination:group_pagination.py:320 Parquet file not found: data/processed/test_run/review_ready.parquet
____________________________ test_import_src_utils _____________________________

    def test_import_src_utils() -> None:
        """Test importing src.utils modules."""
        utils = [
            "src.utils.cache_utils",
            "src.utils.logging_utils",
            "src.utils.path_utils",
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
            "src.utils.state_utils",  # Phase 1.18.1 new
            "src.utils.sort_utils",  # Phase 1.18.1 new
            "src.utils.cli_builder",
            "src.utils.id_utils",
            "src.utils.io_utils",
            "src.utils.mini_dag",
            "src.utils.parallel_utils",
            "src.utils.perf_utils",
            "src.utils.progress",
            "src.utils.resource_monitor",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.validation_utils",
        ]
    
        for util in utils:
            try:
>               importlib.import_module(util)

tests/test_import_audit.py:35: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/Users/joe.j/.pyenv/versions/3.12.2/lib/python3.12/importlib/__init__.py:90: in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
<frozen importlib._bootstrap>:1387: in _gcd_import
    ???
<frozen importlib._bootstrap>:1360: in _find_and_load
    ???
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

name = 'src.utils.validation_utils'
import_ = <function _gcd_import at 0x104b8c0e0>

>   ???
E   ModuleNotFoundError: No module named 'src.utils.validation_utils'

<frozen importlib._bootstrap>:1324: ModuleNotFoundError

During handling of the above exception, another exception occurred:

    def test_import_src_utils() -> None:
        """Test importing src.utils modules."""
        utils = [
            "src.utils.cache_utils",
            "src.utils.logging_utils",
            "src.utils.path_utils",
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
            "src.utils.state_utils",  # Phase 1.18.1 new
            "src.utils.sort_utils",  # Phase 1.18.1 new
            "src.utils.cli_builder",
            "src.utils.id_utils",
            "src.utils.io_utils",
            "src.utils.mini_dag",
            "src.utils.parallel_utils",
            "src.utils.perf_utils",
            "src.utils.progress",
            "src.utils.resource_monitor",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.validation_utils",
        ]
    
        for util in utils:
            try:
                importlib.import_module(util)
            except ImportError as e:
>               pytest.fail(f"Failed to import {util}: {e}")
E               Failed: Failed to import src.utils.validation_utils: No module named 'src.utils.validation_utils'

tests/test_import_audit.py:37: Failed
____________________________ test_absolute_imports _____________________________

    def test_absolute_imports() -> None:
        """Test that all modules can be imported using absolute imports."""
        modules_to_test = [
            # Core modules
            "src.utils.cache_utils",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.io_utils",
            "src.utils.logging_utils",
            "src.utils.parallel_utils",
            "src.utils.path_utils",
            "src.utils.perf_utils",
            "src.utils.resource_monitor",
            "src.utils.sort_utils",
            "src.utils.state_utils",
            "src.utils.validation_utils",
            "src.utils.fragment_utils",  # Phase 1.18.3 addition
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
            "src.alias_matching",
            "src.cleaning",
            "src.disposition",
            "src.grouping",
            "src.manual_io",
            "src.normalize",
            "src.performance",
            "src.salesforce",
            "src.similarity",
            "src.survivorship",
            # App modules
            "app.components.controls",
            "app.components.export",
            "app.components.group_details",
            "app.components.group_list",
            "app.components.maintenance",
            "app.main",
        ]
    
        for module_name in modules_to_test:
            try:
>               module = __import__(module_name, fromlist=[""])
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E               ModuleNotFoundError: No module named 'src.utils.validation_utils'

tests/test_imports.py:49: ModuleNotFoundError

During handling of the above exception, another exception occurred:

    def test_absolute_imports() -> None:
        """Test that all modules can be imported using absolute imports."""
        modules_to_test = [
            # Core modules
            "src.utils.cache_utils",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.io_utils",
            "src.utils.logging_utils",
            "src.utils.parallel_utils",
            "src.utils.path_utils",
            "src.utils.perf_utils",
            "src.utils.resource_monitor",
            "src.utils.sort_utils",
            "src.utils.state_utils",
            "src.utils.validation_utils",
            "src.utils.fragment_utils",  # Phase 1.18.3 addition
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
            "src.alias_matching",
            "src.cleaning",
            "src.disposition",
            "src.grouping",
            "src.manual_io",
            "src.normalize",
            "src.performance",
            "src.salesforce",
            "src.similarity",
            "src.survivorship",
            # App modules
            "app.components.controls",
            "app.components.export",
            "app.components.group_details",
            "app.components.group_list",
            "app.components.maintenance",
            "app.main",
        ]
    
        for module_name in modules_to_test:
            try:
                module = __import__(module_name, fromlist=[""])
                assert module is not None
            except ImportError as e:
>               pytest.fail(f"Failed to import {module_name}: {e}")
E               Failed: Failed to import src.utils.validation_utils: No module named 'src.utils.validation_utils'

tests/test_imports.py:52: Failed
______________________________ test_utils_imports ______________________________

    def test_utils_imports() -> None:
        """Test that all utility modules can be imported."""
        utils_modules = [
            "src.utils.cache_utils",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.io_utils",
            "src.utils.logging_utils",
            "src.utils.parallel_utils",
            "src.utils.path_utils",
            "src.utils.perf_utils",
            "src.utils.resource_monitor",
            "src.utils.sort_utils",
            "src.utils.state_utils",
            "src.utils.validation_utils",
            "src.utils.fragment_utils",  # Phase 1.18.3 addition
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
        ]
    
        for module_name in utils_modules:
            try:
>               module = __import__(module_name, fromlist=[""])
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
E               ModuleNotFoundError: No module named 'src.utils.validation_utils'

tests/test_imports.py:94: ModuleNotFoundError

During handling of the above exception, another exception occurred:

    def test_utils_imports() -> None:
        """Test that all utility modules can be imported."""
        utils_modules = [
            "src.utils.cache_utils",
            "src.utils.dtypes",
            "src.utils.hash_utils",
            "src.utils.io_utils",
            "src.utils.logging_utils",
            "src.utils.parallel_utils",
            "src.utils.path_utils",
            "src.utils.perf_utils",
            "src.utils.resource_monitor",
            "src.utils.sort_utils",
            "src.utils.state_utils",
            "src.utils.validation_utils",
            "src.utils.fragment_utils",  # Phase 1.18.3 addition
            # "src.utils.ui_helpers",  # Deprecated - moved to deprecated/ folder
        ]
    
        for module_name in utils_modules:
            try:
                module = __import__(module_name, fromlist=[""])
                assert module is not None
            except ImportError as e:
>               pytest.fail(f"Failed to import utility {module_name}: {e}")
E               Failed: Failed to import utility src.utils.validation_utils: No module named 'src.utils.validation_utils'

tests/test_imports.py:97: Failed
________ TestInterruptResumeIntegration.test_interrupt_resume_workflow _________

self = <tests.test_interrupt_resume.TestInterruptResumeIntegration object at 0x129d2e4b0>

        def test_interrupt_resume_workflow(self) -> None:
            """Test the complete interrupt and resume workflow."""
            # Create test CSV
            csv_path = self.create_test_csv("test_interrupt.csv", 5)
    
            try:
                # Create temporary directories
                temp_dir = tempfile.mkdtemp()
                interim_dir = os.path.join(temp_dir, "interim")
                processed_dir = os.path.join(temp_dir, "processed")
                os.makedirs(interim_dir)
                os.makedirs(processed_dir)
    
                # Create a minimal config
                config_path = os.path.join(temp_dir, "test_config.yaml")
                with open(config_path, "w") as f:
                    f.write(
                        """
    similarity:
      high: 92
      medium: 84
      penalty:
        suffix_mismatch: 25
        num_style_mismatch: 5
    parallelism:
      workers: 1
      backend: "threading"
      chunk_size: 1000
      small_input_threshold: 1000
    """,
                    )
    
                # Test that we can run a small pipeline
                cmd = [
                    sys.executable,
                    "src/cleaning.py",
                    "--input",
                    csv_path,
                    "--outdir",
                    processed_dir,
                    "--config",
                    config_path,
                    "--workers",
                    "1",
                    "--parallel-backend",
                    "threading",
                    "--no-resume",
                ]
    
                # Run the pipeline (should complete quickly with small data)
                result = subprocess.run(
                    cmd, check=False, capture_output=True, text=True, timeout=30
                )
    
                # Should complete successfully
>               assert result.returncode == 0, f"Pipeline failed: {result.stderr}"
E               AssertionError: Pipeline failed: Traceback (most recent call last):
E                   File "/Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/cleaning.py", line 24, in <module>
E                     from src.alias_matching import (
E                 ModuleNotFoundError: No module named 'src'
E                 
E               assert 1 == 0
E                +  where 1 = CompletedProcess(args=['/Users/joe.j/.pyenv/versions/3.12.2/bin/python3.12', 'src/cleaning.py', '--input', '/var/folde...aning.py", line 24, in <module>\n    from src.alias_matching import (\nModuleNotFoundError: No module named \'src\'\n').returncode

tests/test_interrupt_resume.py:165: AssertionError
____________________________ test_validate_csv_file ____________________________

    def test_validate_csv_file():
        """Test CSV file validation."""
        # Test with valid file
>       assert validate_csv_file("test.csv") is True
E       AssertionError: assert False is True
E        +  where False = validate_csv_file('test.csv')

tests/test_io_utils.py:350: AssertionError
------------------------------ Captured log call -------------------------------
ERROR    src.utils.io_utils:io_utils.py:435 CSV validation failed for test.csv: [Errno 2] No such file or directory: 'test.csv'
____________________________ test_track_memory_peak ____________________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x12ddc1d30>

    def test_track_memory_peak(caplog: pytest.LogCaptureFixture) -> None:
        """Test memory peak tracking context manager."""
        # Set up logging to capture messages
        caplog.set_level(logging.INFO)
        logger = logging.getLogger("test")
    
        with track_memory_peak("test_stage", logger):
            # Simulate some memory usage
            pass
    
        # Should log memory peak
>       assert "Memory peak at 'test_stage'" in caplog.text
E       assert "Memory peak at 'test_stage'" in ''
E        +  where '' = <_pytest.logging.LogCaptureFixture object at 0x12ddc1d30>.text

tests/test_perf_utils.py:27: AssertionError
_______________________________ test_time_stage ________________________________

caplog = <_pytest.logging.LogCaptureFixture object at 0x12dd72c30>

    def test_time_stage(caplog: pytest.LogCaptureFixture) -> None:
        """Test stage timing context manager."""
        # Set up logging to capture messages
        caplog.set_level(logging.INFO)
        logger = logging.getLogger("test")
    
        with time_stage("test_stage", logger):
            # Simulate some work
            time.sleep(0.01)
    
        # Should log timing
>       assert "Stage 'test_stage' completed in" in caplog.text
E       assert "Stage 'test_stage' completed in" in 'INFO     test:perf_utils.py:249 [stage:start] test_stage\nINFO     test:perf_utils.py:254 [stage:end] test_stage (0.01s)\n'
E        +  where 'INFO     test:perf_utils.py:249 [stage:start] test_stage\nINFO     test:perf_utils.py:254 [stage:end] test_stage (0.01s)\n' = <_pytest.logging.LogCaptureFixture object at 0x12dd72c30>.text

tests/test_perf_utils.py:41: AssertionError
------------------------------ Captured log call -------------------------------
INFO     test:perf_utils.py:249 [stage:start] test_stage
INFO     test:perf_utils.py:254 [stage:end] test_stage (0.01s)
___________ TestReadOnlySafety.test_no_destructive_functions_in_code ___________

self = <tests.test_readonly_safety.TestReadOnlySafety object at 0x129be7f20>

    def test_no_destructive_functions_in_code(self) -> None:
        """Test that no destructive functions are called in the codebase."""
        destructive_functions = find_destructive_functions()
    
        # Filter out legitimate uses (like in tests or cleanup tools)
        legitimate_uses = set()
        for func in destructive_functions:
            file_path = func.split(":")[0]
            if any(legit in file_path for legit in ["test_", "cleanup_", "tools/"]):
                legitimate_uses.add(func)
    
        # Remove legitimate uses from the set
        problematic_functions = destructive_functions - legitimate_uses
    
>       assert len(problematic_functions) == 0, (
            f"Found potentially destructive functions in production code:\n"
            f"{chr(10).join(sorted(problematic_functions))}\n"
            f"All destructive operations must be gated behind Phase-1 fuses."
        )
E       AssertionError: Found potentially destructive functions in production code:
E         app/components/maintenance.py:213:preview_delete_runs
E         app/components/maintenance.py:233:delete_runs
E         scripts/run_modes_benchmark.py:347:shutil.rmtree
E         All destructive operations must be gated behind Phase-1 fuses.
E       assert 3 == 0
E        +  where 3 = len({'app/components/maintenance.py:213:preview_delete_runs', 'app/components/maintenance.py:233:delete_runs', 'scripts/run_modes_benchmark.py:347:shutil.rmtree'})

tests/test_readonly_safety.py:231: AssertionError
__________ TestReadOnlySafety.test_maintenance_ui_shows_readonly_copy __________

self = <tests.test_readonly_safety.TestReadOnlySafety object at 0x129b701d0>

    def test_maintenance_ui_shows_readonly_copy(self) -> None:
        """Test that maintenance UI shows the correct read-only message."""
>       assert (
            check_maintenance_ui_copy()
        ), "Maintenance UI must show: 'Run deletion functionality will be implemented in a future phase.'"
E       AssertionError: Maintenance UI must show: 'Run deletion functionality will be implemented in a future phase.'
E       assert False
E        +  where False = check_maintenance_ui_copy()

tests/test_readonly_safety.py:259: AssertionError
___________ TestReadOnlySafety.test_maintenance_rendered_in_sidebar ____________

self = <tests.test_readonly_safety.TestReadOnlySafety object at 0x129b70380>

    def test_maintenance_rendered_in_sidebar(self) -> None:
        """Test that maintenance is rendered in sidebar context."""
>       assert (
            check_sidebar_placement()
        ), "Maintenance component must be rendered in sidebar using st.sidebar.subheader"
E       AssertionError: Maintenance component must be rendered in sidebar using st.sidebar.subheader
E       assert False
E        +  where False = check_sidebar_placement()

tests/test_readonly_safety.py:265: AssertionError
______________________ test_header_list_raises_typeerror _______________________

monkeypatch = <_pytest.monkeypatch.MonkeyPatch object at 0x12f578200>

    def test_header_list_raises_typeerror(monkeypatch: pytest.MonkeyPatch) -> None:
        def fake_parallel(*args, **kwargs):
            return ["id_a", "id_b", "score", "ratio_name", "ratio_set"]
    
>       monkeypatch.setattr(sim, "_compute_similarity_scores_parallel", fake_parallel)
E       AttributeError: <module 'src.similarity' from '/Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/similarity/__init__.py'> has no attribute '_compute_similarity_scores_parallel'

tests/test_similarity_header_list_regression.py:11: AttributeError
=============================== warnings summary ===============================
tests/perf/test_groups_bench.py:214
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:214: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:241
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:241: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:269
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:269: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:301
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:301: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:333
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:333: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:334
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:334: PytestUnknownMarkWarning: Unknown pytest.mark.slow - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.slow

tests/perf/test_groups_bench.py:361
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:361: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:362
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:362: PytestUnknownMarkWarning: Unknown pytest.mark.slow - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.slow

tests/perf/test_groups_bench.py:393
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:393: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:426
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:426: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:463
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:463: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/perf/test_groups_bench.py:499
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/perf/test_groups_bench.py:499: PytestUnknownMarkWarning: Unknown pytest.mark.performance - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.performance

tests/test_alias_validation.py:189
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_alias_validation.py:189: PytestUnknownMarkWarning: Unknown pytest.mark.slow - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytest.param(15000, marks=pytest.mark.slow, id="slow"),

tests/test_cache_keys.py:11
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_keys.py:11: PytestUnknownMarkWarning: Unknown pytest.mark.duckdb - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytestmark = [pytest.mark.duckdb, pytest.mark.pyarrow]

tests/test_cache_keys.py:11
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_cache_keys.py:11: PytestUnknownMarkWarning: Unknown pytest.mark.pyarrow - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytestmark = [pytest.mark.duckdb, pytest.mark.pyarrow]

tests/test_duckdb_query_params.py:13
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/tests/test_duckdb_query_params.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.duckdb - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    pytestmark = [pytest.mark.duckdb]

tests/test_disposition_vectorized_phase1353.py::test_vectorized_vs_legacy_identical_output
tests/test_disposition_vectorized_phase1353.py::test_disposition_classification_correctness
tests/test_disposition_vectorized_phase1353.py::test_feature_flag_rollback
tests/test_disposition_vectorized_phase1353.py::test_performance_improvement
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/disposition.py:570: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.
    suspicious_singleton_mask = name_series.str.contains(

tests/test_disposition_vectorized_phase1353.py::test_vectorized_vs_legacy_identical_output
tests/test_disposition_vectorized_phase1353.py::test_disposition_classification_correctness
tests/test_disposition_vectorized_phase1353.py::test_feature_flag_rollback
tests/test_disposition_vectorized_phase1353.py::test_performance_improvement
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/disposition.py:861: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.
    suspicious_singleton_mask = name_series.str.contains(

tests/test_id_utils.py::TestNormalizeSfidSeries::test_normalize_sfid_series_handles_nan
  /Users/joe.j/Documents/dev/salesforce/apps/company_junction/src/utils/id_utils.py:110: FutureWarning: Operation between non boolean Series with different indexes will no longer return a boolean result in a future version. Cast both Series to object type to maintain the prior behavior.
    out.loc[non_empty & is15] = s_filtered[is15].map(sfid15_to_18)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
- generated xml file: /Users/joe.j/Documents/dev/salesforce/apps/company_junction/artifacts/qa/junit_after.xml -
================================ tests coverage ================================
_______________ coverage: platform darwin, python 3.12.2-final-0 _______________

Name                                 Stmts   Miss  Cover   Missing
------------------------------------------------------------------
src/__init__.py                          0      0   100%
src/alias_matching.py                  293    115    61%   46, 82-83, 130-134, 138-139, 141, 145, 163, 168-170, 176, 179-268, 387, 396, 407, 442-450, 477-478, 505-509, 563, 624-628, 633-634, 637, 649, 681, 698-701, 720-744, 756-785, 798-804
src/cleaning.py                        618    572     7%   110-117, 138-189, 212-282, 299, 325-333, 347-392, 434-1642, 1647-1767, 1771
src/disposition.py                     317     36    89%   86, 91, 103, 125-127, 150-152, 214, 276, 325, 373, 392, 631, 658, 672-676, 682, 714-715, 728-734, 761, 766-767, 771, 778
src/dtypes_map.py                       14      0   100%
src/grouping.py                        308     90    71%   166-174, 189, 208, 213, 217, 220-225, 246-247, 261-262, 278-281, 308-312, 316-323, 329-351, 366, 368, 370, 377-378, 382-383, 400-423, 429-432, 450-452, 484, 508, 520-524, 564-569, 582, 611, 654, 656, 678-681, 698, 726
src/manual_io.py                        83     46    45%   18-20, 34-54, 77-79, 96-101, 136-145, 162-168, 191-202, 219-225, 242-244
src/normalize.py                       205     35    83%   243-244, 338, 345, 359-364, 378, 421, 425-443, 458-462, 480-481, 517-519, 600
src/performance.py                      78     56    28%   21-22, 31-36, 40-46, 50-54, 58-59, 63-72, 93-96, 149-155, 168-191, 208-221
src/similarity/__init__.py              62     23    63%   52-60, 85-86, 98-99, 119-121, 132-141, 149-150
src/similarity/blocking.py             184     53    71%   77, 137, 149, 176-189, 257, 275-297, 334-345, 360-364, 377-407, 413-425
src/similarity/diagnostics.py           51      7    86%   32-40, 100
src/similarity/scoring.py              101      6    94%   149-173
src/similarity/types.py                 10      0   100%
src/survivorship.py                    236    137    42%   17-18, 47-55, 62-63, 80-88, 125, 162-169, 207-265, 289, 402-438, 455-479, 500-586, 605-672, 683-684, 697-703
src/utils/__init__.py                    8      0   100%
src/utils/artifact_management.py        15      2    87%   31-32
src/utils/cache_keys.py                 69      8    88%   44, 46, 102-103, 142-143, 187-188
src/utils/cache_utils.py               342    298    13%   36-40, 49-65, 70-72, 77-87, 93, 99-101, 106-112, 122-146, 154-156, 163-197, 202-224, 229-262, 267-290, 303-356, 369-459, 469-491, 496-519, 524-538, 543-548, 561-599, 612-634, 639-650
src/utils/cli_builder.py                86      8    91%   90-98, 173-174
src/utils/dtypes.py                     81     14    83%   17-18, 51-54, 65-69, 129, 204, 211, 226-227
src/utils/duckdb_group_stats.py        132     20    85%   89-91, 221-222, 272-274, 320-378
src/utils/duckdb_utils.py               29     15    48%   14-22, 27-30, 39-42
src/utils/exact_equals.py              105     32    70%   84-85, 124, 222-267, 292
src/utils/filtering.py                  95     24    75%   73-91, 144, 161, 180-185, 223-228, 234-239
src/utils/fragment_utils.py             12      0   100%
src/utils/group_details.py             231     89    61%   107-110, 120-122, 124-129, 151-154, 185-188, 205-328, 342, 351, 364-369, 435, 454, 463-464, 474, 481, 492, 501-506
src/utils/group_pagination.py          478    327    32%   102-110, 120-123, 130-131, 175-178, 197-200, 203-214, 217-229, 252-255, 263-274, 285-288, 324-487, 511-725, 764, 771-772, 781, 820, 905-910, 924-956, 973-1031, 1048-1106
src/utils/group_stats.py                46     18    61%   42, 57, 63, 112-157
src/utils/hash_utils.py                 47     29    38%   34-65, 79-80, 103, 121, 179-181
src/utils/id_utils.py                   50      1    98%   96
src/utils/io_utils.py                  145     29    80%   101-103, 127, 140-145, 158-162, 184-191, 249-251, 338-340, 359-360, 407
src/utils/logging_utils.py               5      1    80%   13
src/utils/metrics.py                    56     39    30%   14-75, 93-96, 101-107, 115, 120-123, 128-131, 136-139, 144-147
src/utils/mini_dag.py                  235     43    82%   106, 110, 150-153, 155, 158, 180-183, 194-195, 209-221, 247, 262-265, 269, 287-291, 324-327, 339-341, 404, 446-448, 454-455, 471-472
src/utils/opt_deps.py                   21      0   100%
src/utils/parallel_protocols.py          8      0   100%
src/utils/parallel_utils.py            247     90    64%   35-38, 47-67, 72-77, 85-87, 180-214, 223-224, 362, 396-424, 516-529, 552-560, 632-638
src/utils/parity_validator.py          126     33    74%   67, 69, 72-74, 82-84, 113, 175, 196-233, 264, 284-285, 314-316, 351-352, 370-372
src/utils/parquet_size_reporter.py     149    132    11%   30, 42-102, 106-140, 144-154, 158-198, 202-221, 240-288, 298-322, 327, 341-396
src/utils/path_utils.py                 99     47    53%   16, 26, 36-37, 84, 103, 105, 127, 129, 142-163, 213-222, 227-240
src/utils/perf_utils.py                 94     51    46%   28-29, 62, 65, 74-80, 105-148, 170-199, 212-228
src/utils/pipeline_constants.py         10      0   100%
src/utils/progress.py                   64     21    67%   35-40, 44, 47, 60-65, 71, 73-75, 81-84
src/utils/resource_monitor.py          103     13    87%   19-21, 58, 129-131, 164-166, 187-188, 236
src/utils/run_management.py            112    112     0%   6-302
src/utils/schema_utils.py              229     25    89%   147-153, 270, 402, 427, 469-470, 493-502, 528, 541-566
src/utils/settings.py                   35      1    97%   62
src/utils/simple_state.py               56     56     0%   7-209
src/utils/sort_utils.py                 28      0   100%
src/utils/sql_utils.py                   8      0   100%
src/utils/state_utils.py                81      1    99%   184
src/utils/ui_session.py                 30     12    60%   22-29, 39-41, 50-51
src/utils/union_find.py                 57     57     0%   7-174
------------------------------------------------------------------
TOTAL                                 6384   2824    56%
Coverage XML written to file artifacts/qa/coverage_after.xml
=========================== short test summary info ============================
FAILED tests/contracts/test_parquet_contracts.py::TestParquetContracts::test_required_columns_exist
FAILED tests/contracts/test_parquet_contracts.py::TestParquetContracts::test_schema_consistency
FAILED tests/contracts/test_parquet_contracts.py::TestParquetContracts::test_column_types
FAILED tests/contracts/test_parquet_contracts.py::TestParquetContracts::test_no_extra_required_columns
FAILED tests/contracts/test_parquet_contracts.py::TestParquetContracts::test_contract_evolution
FAILED tests/contracts/test_parquet_contracts.py::TestHardcodedColumnAssumptions::test_detect_schema_constants_usage
FAILED tests/contracts/test_parquet_contracts.py::TestLegacyColumnHandling::test_duckdb_handles_present_legacy_columns
FAILED tests/contracts/test_parquet_contracts.py::TestLegacyColumnHandling::test_pyarrow_handles_present_legacy_columns
FAILED tests/contracts/test_parquet_contracts.py::TestLegacyColumnHandling::test_conditional_filtering_works_with_present_columns
FAILED tests/lints/test_no_schema_fragile_hardcoding.py::TestNoSchemaFragileHardcoding::test_no_hardcoded_primary_name_in_details_context
FAILED tests/lints/test_no_schema_fragile_hardcoding.py::TestNoSchemaFragileHardcoding::test_no_hardcoded_weakest_edge_without_availability_check
FAILED tests/lints/test_no_schema_fragile_hardcoding.py::TestNoSchemaFragileHardcoding::test_no_hardcoded_is_primary_without_availability_check
FAILED tests/test_alias_matching_parallelism.py::TestAliasMatchingParallelism::test_parallel_executor_integration
FAILED tests/test_cleaning.py::TestCleaning::test_validate_required_columns
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_basic
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_with_parallelism
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_no_parallel
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_with_run_control
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_with_extra_args
FAILED tests/test_cli_builder.py::TestBuildCliCommand::test_build_cli_command_custom_outdir
FAILED tests/test_details_fast_path.py::TestGroupDetailsDuckDB::test_get_group_details_duckdb_file_not_found
FAILED tests/test_details_fast_path.py::TestGroupsRouting::test_groups_use_duckdb_when_stats_parquet_exists
FAILED tests/test_details_fast_path.py::TestGroupsRouting::test_groups_fallback_to_pyarrow_when_duckdb_unavailable
FAILED tests/test_disposition.py::TestDisposition::test_manual_override_application
FAILED tests/test_disposition.py::TestDisposition::test_multiple_names_verification
FAILED tests/test_duckdb_group_stats_phase1354.py::test_duckdb_memoization - ...
FAILED tests/test_duckdb_group_stats_phase1354.py::test_duckdb_parquet_write
FAILED tests/test_duckdb_group_stats_phase1354.py::test_parquet_size_reporter
FAILED tests/test_duckdb_group_stats_phase1354.py::test_performance_improvement
FAILED tests/test_duckdb_query_params.py::test_no_f_string_sql_queries - Fail...
FAILED tests/test_groups_pagination.py::TestGroupStatsComputation::test_compute_group_stats_no_primary
FAILED tests/test_groups_pagination.py::TestFilterApplication::test_apply_filters_pyarrow_edge_strength
FAILED tests/test_groups_pagination.py::TestPaginationLimits::test_pagination_limits_page_bounds
FAILED tests/test_groups_pagination.py::TestSortingStability::test_sorting_stability_group_id_tiebreaker
FAILED tests/test_import_audit.py::test_import_src_utils - Failed: Failed to ...
FAILED tests/test_imports.py::test_absolute_imports - Failed: Failed to impor...
FAILED tests/test_imports.py::test_utils_imports - Failed: Failed to import u...
FAILED tests/test_interrupt_resume.py::TestInterruptResumeIntegration::test_interrupt_resume_workflow
FAILED tests/test_io_utils.py::test_validate_csv_file - AssertionError: asser...
FAILED tests/test_perf_utils.py::test_track_memory_peak - assert "Memory peak...
FAILED tests/test_perf_utils.py::test_time_stage - assert "Stage 'test_stage'...
FAILED tests/test_readonly_safety.py::TestReadOnlySafety::test_no_destructive_functions_in_code
FAILED tests/test_readonly_safety.py::TestReadOnlySafety::test_maintenance_ui_shows_readonly_copy
FAILED tests/test_readonly_safety.py::TestReadOnlySafety::test_maintenance_rendered_in_sidebar
FAILED tests/test_similarity_header_list_regression.py::test_header_list_raises_typeerror
ERROR tests/perf/test_groups_bench.py::TestGroupsPageBenchmarks::test_groups_page_10k_pyarrow
ERROR tests/perf/test_groups_bench.py::TestGroupsPageBenchmarks::test_groups_page_10k_duckdb
ERROR tests/perf/test_groups_bench.py::TestGroupsPageBenchmarks::test_groups_page_100k_pyarrow
ERROR tests/perf/test_groups_bench.py::TestGroupsPageBenchmarks::test_groups_page_100k_duckdb
ERROR tests/perf/test_groups_bench.py::TestGroupsPageBenchmarks::test_groups_page_1m_pyarrow
ERROR tests/perf/test_groups_bench.py::TestGroupsPageBenchmarks::test_groups_page_1m_duckdb
ERROR tests/perf/test_groups_bench.py::TestGroupDetailsBenchmarks::test_group_details_10k_pyarrow
ERROR tests/perf/test_groups_bench.py::TestGroupDetailsBenchmarks::test_group_details_10k_duckdb
ERROR tests/perf/test_groups_bench.py::TestSortVariants::test_sort_variants_10k
ERROR tests/perf/test_groups_bench.py::TestFilterVariants::test_filter_variants_10k
ERROR tests/test_cache_utils.py::test_latest_pointer_operations
====== 45 failed, 734 passed, 19 skipped, 25 warnings, 11 errors in 9.05s ======
