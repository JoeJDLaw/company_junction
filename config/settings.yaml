# Company Junction Pipeline Configuration

# Data Processing Settings
data:
  # Default name column for duplicate detection
  name_column: "Account Name"
  
  # Supported file formats
  supported_formats: [".csv", ".xlsx", ".xls", ".json", ".xml"]
  
  # Output file pattern
  output_pattern: "cleaned_{object_type}_{timestamp}.csv"

# Ingest Configuration
ingest:
  name_synonyms: ["account name", "name", "company", "company name", "legal name", "organization", "org name"]
  id_synonyms: ["account id", "id", "sfid", "external id", "uuid", "guid", "record id"]
  use_input_disposition: false
  preserve_input_column_order: true

# Compatibility Settings
compatibility:
  suppress_metadata_columns: false    # If true, do NOT add any new metadata columns beyond the three above
  force_salesforce_mode: false        # Emergency fallback if any regressions are detected

# Observability Settings
observability:
  structured_logs: true               # Emit one JSON line summarizing mappings per run
  dry_run_ingest: false               # If true, run ingest mapping and logging but do not proceed to heavy stages

# Pipeline Configuration
pipeline:
  exact_equals_first_pass:
    enable: true
    input_name_col: "Account Name"
    min_group_size: 2
    key_trim: true
    representative_policy: "min_account_id"
    pairs_emission: "spanning_tree"  # "spanning_tree" | "complete"

# Filtering Configuration
filtering:
  write_filtered_out: true
  filtered_out_columns: ["account_id", "account_name", "reason"]

similarity:
  high: 92
  medium: 84
  penalty:
    suffix_mismatch: 25
    num_style_mismatch: 5
    punctuation_mismatch: 3
  max_alias_pairs: 100000
  # Soft-ban blocking strategy (only strategy supported)
  blocking:
    allowlist_tokens: ["99", "7", "24", "1-800", "360", "1", "2", "3", "4", "5", "6", "8", "9", "10", "pnc"]
    allowlist_bigrams: ["99 cents", "7 eleven", "24 hour", "1-800 got", "360 behavioral"]
    denylist_tokens: ["the", "and", "of", "for", "in", "on", "at", "to", "from", "a", "an", "as", "by", "is", "it", "or", "be", "are", "was", "were", "been", "being", "have", "has", "had", "do", "does", "did", "will", "would", "could", "should", "may", "might", "must", "can", "shall"]
    stop_tokens: ["inc", "llc", "ltd"]  # Tokens to skip in first-token blocking
    soft_ban:
      shard_strategy: "second_token"  # "second_token" | "third_token_initial" | "char_trigram"
      fallback_shard: "char_trigram"  # Fallback when second token still too large
      max_shard_size: 200  # Shard again if still > 200
      char_bigram_gate: 0.1  # Simple overlap ratio gate (0..1)
      length_window: 150  # |len(a)-len(b)| <= 150 (wider to include long 'PNC Not Sure' variants)
      min_token_overlap: 1  # Require at least 1 shared token
      max_candidates_per_record: 50  # Cap pairs per record
      block_cap: 800  # Still honored for jumbo blocks
    # Brand suggestions configuration
    min_suggestion_count: 10  # Minimum token frequency to suggest
    min_singleton_pct: 0.6    # Minimum singleton rate to suggest

  scoring:
    use_bulk_cdist: true  # NEW: use RapidFuzz bulk scoring
    gate_cutoff: 72  # NEW: token_set_ratio gate cutoff
  
  # Distractor guardrails to prevent false-positive groupings
  distractor_guardrails:
    enabled: false  # Default off - flip to true to enable guardrails
    strict_mode: false  # Optional ramp - when true, applies stricter penalties
    
    # Distractor token categories and penalty weights
    distractor_tokens:
      bank: ["bank", "banking", "financial", "credit", "loan"]
      venue: ["park", "field", "stadium", "arena", "center", "theater", "theatre"]
      brand_extension: ["eats", "web services", "aws", "cloud", "payments"]
      generic: ["services", "solutions", "group", "holdings", "ventures"]
    
    # Penalty weights (0-100, where 100 = complete veto)
    penalty_weights:
      bank: 50      # Moderate penalty for bank distractors
      venue: 75     # Strong penalty for venue distractors  
      brand_extension: 40  # Moderate penalty for brand extensions
      generic: 20   # Light penalty for generic terms
    
    # Non-distractor evidence requirements
    evidence_requirements:
      min_non_distractor_tokens: 2  # Require ≥2 shared non-distractor tokens
      strong_corroboration_threshold: 90  # Char-sim threshold for 1-token + corroboration
      require_suffix_match_for_corroboration: true  # Require suffix match for 1-token cases
  
  # NEW: Enhanced normalization for better retail brand matching
  normalization:
    weak_tokens: ["only", "the", "and", "of", "for", "in", "on", "at", "to", "from"]
    plural_singular_map:
      stores: store
      services: service
      holdings: holding
      companies: company
      shops: shop
      centers: center
      offices: office
      locations: location
    canonical_retail_terms:
      store: store
      stores: store
      shop: store
      shops: store
    enable_plural_normalization: true
    enable_weak_token_filtering: true
    enable_canonical_retail_terms: true

grouping:
  edge_gating:
    enabled: true
    require_high_to_primary: false
    allow_medium_plus_shared_token: true
    shared_token_source: "name_core_tokens"  # exclude stop tokens
    canopy_bound:
      enabled: true
      max_without_high_edge: 8
    performance:
      vectorize_edge_scores: true  # NEW: build dict without iterrows
      token_parse: auto  # NEW: 'auto'|'json'|'list'; auto skips parse if already list
      maintain_unionfind_size: true  # NEW: enable size[] for canopy checks
      pair_columns: [id_a, id_b, score]  # NEW: restrict columns during sort to reduce copy

llm:
  enabled: false
  delete_threshold: 85

survivorship:
  tie_breakers: [created_date, account_id]
  optimized: true   # controls the hybrid fast-path
  performance:
    vectorized: true  # NEW: enable vectorized primary selection
    generate_preview_by_group: true  # NEW: generate per-group previews
    skip_clean_groups: true  # NEW: skip groups with no conflicts
  preview_output: "survivorship_preview.parquet"  # NEW: output path for previews

io:
  interim_format: parquet
  use_arrow_strings: false  # DISABLED: PyArrow backend removed, using DuckDB + pandas strings
  # Phase 1.35.4: Parquet I/O configuration
  parquet:
    io_backend: "duckdb"  # "duckdb" | "pyarrow" (default: duckdb for new writes)
    compression: "zstd"
    row_group_size: 128000
    dictionary_compression: true
    statistics: true
    target_size_mb: 180  # Target size for review parquet

disposition:
  performance:
    vectorized: true  # Phase 1.35.3: enable np.select path
    compile_token_regex_once: true  # Phase 1.35.3: compile regex once
    suspicious_singleton_regex: "(?i)\\b(unknown|unsure|not sure|no idea|test|sample|example|dummy|temp|temporary|temp agency|none|n/?a|tbd|delete|remove|do not use)\\b"
  # Phase 1.35.3: Moved hardcoded blacklist to configuration
  # Blacklist behavior:
  # - No disposition.blacklist key → built-ins + manual terms
  # - Key present (even empty lists) → config terms + manual only (no built-ins, no heuristics)
  # - Per-process caches; clear_blacklist_cache() resets during long runs
  blacklist:
    tokens: [
      "temp", "temporary", "unknown", "na", "n/a", "tbd", "test", "sample",
      "paystub", "employees", "delete", "unsure"
    ]
    phrases: [
      "pnc is not sure", "pnc is unsure", "no paystub", "no paystubs",
      "1099", "1099 pnc", "none", "do not use", "not sure",
      "unknown company", "no company", "no employer"
    ]
  manual_overrides:
    enabled: true
    key_column: "account_id"  # adjust to actual id column if not index

# UI Performance Configuration
ui_perf:
  groups:
    use_stats_parquet: true         # rollback toggle
    duckdb_prefer_over_pyarrow: true
    rows_duckdb_threshold: 30000
    groups_duckdb_threshold: 10000
  details:
    use_details_parquet: true       # rollback toggle
    allow_pyarrow_fallback: true    # enable fallback for group details
    lru_capacity: 16
    auto_load_on_expand: true       # auto-load details when expander opens
    show_load_button: false         # hide button when auto-load enabled

# UI Configuration
ui:
  sort:
    default: "group_size DESC"      # Default sort when unknown sort key provided
  use_duckdb_for_groups: true      # Force DuckDB routing for groups list
  similarity_slider:
    enable: true
    control: "stepper"              # plus/minus control, not free slider
    default_bucket: "100"           # "100" | "95" | "92" | "90"
    buckets: [100, 95, 92, 90]
    min: 90                         # lower bound for stepper
    max: 100                        # upper bound for stepper
    step: 1                         # increment for +/- buttons
  page_size_default: 50            # Emergency reduction from 500
  timeout_seconds: 30              # Timeout for UI operations
  cache_capacity: 16               # LRU cache capacity for details
  duckdb_threads: 4                # Number of DuckDB threads
  max_pyarrow_group_stats_seconds: 5  # Auto-switch threshold for group stats
  # Admin and maintenance features
  admin_mode: true                 # Enable admin features like run deletion
  enable_run_deletion: true        # Enable run deletion functionality

# Salesforce Integration
salesforce:
  # Default object types
  object_types: ["Account", "Contact", "Lead", "Opportunity"]
  
  # Batch size for API operations
  batch_size: 200
  
  # Retry settings for failed operations
  max_retries: 3
  retry_delay: 5

# Engine Configuration
engine:
  duckdb:
    enabled: true  # DuckDB is the single engine for non-pandas work
    threads: auto  # auto-detect optimal thread count
    preserve_order: true  # deterministic ordering where needed
    # Phase 1.35.4: Enhanced DuckDB configuration
    memory_limit: null  # Use DUCKDB_MEMORY_LIMIT env var or DuckDB default
    pragmas:
      enable_object_cache: true
      preserve_insertion_order: true
    parquet:
      compression: "zstd"
      row_group_size: 128000
      dictionary_compression: true
      statistics: true

# Engine Selection Configuration
engines:
  # Backend selection for specific stages
  filtering: "auto"  # "auto" | "pandas" | "duckdb"
  exact_equals: "auto"  # "auto" | "pandas" | "duckdb"
  
  # Automatic selection thresholds
  duckdb_threshold_rows: 50000  # Use DuckDB for datasets >= this size
  
  # Feature flags for unstable DuckDB implementations
  enable_unstable_duckdb_paths: false  # Enable experimental DuckDB implementations

# Parallelism Configuration
parallelism:
  # Default number of workers (None for auto-detection)
  workers: auto  # NEW: resolve to min(8, cpu_count)
  
  # Backend to use ('loky' for processes, 'threading' for threads)
  backend: "loky"  # prefer processes
  
  # Chunk size for parallel processing
  chunk_size: 1000
  
  # Chunk size for pair processing (NEW)
  chunk_size_pairs: 300000  # NEW: hint for execute_chunked
  
  # Threshold for auto-switching to sequential execution
  small_input_threshold: 10000
  
  # Memory cap percentage (0.75 = 75% of total RAM)
  memory_cap_percent: 75.0

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "pipeline.log"
  contract:
    enable: true
    required_fields: ["stage", "backend", "config_digest", "request_id"]
    prefix_format: "{stage} | backend={backend}"

# Cleanup Configuration
cleanup:
  protect_latest: true
  pinned_runs: []
  max_age_days: null  # no default age limit
  keep_at_least: 1  # minimum runs to keep (0 allows empty state)
  allow_empty_state: false  # feature flag for empty state support

# CSV Processing Configuration
csv:
  # Engine preference for CSV reading (auto, pyarrow, c, python)
  engine: "auto"
  
  # Number of rows to sample for schema inference
  sample_rows: 10000
  
  # Columns to force to string type
  force_string_cols:
    - "account_id"
    - "parent_account_id"

# File Paths
paths:
  raw_data: "data/raw"
  interim_data: "data/interim"
  processed_data: "data/processed"
  test_fixtures: "tests/fixtures"


# Phase 1.35.4: Group Stats Configuration
group_stats:
  backend: "duckdb"  # "duckdb" | "pandas" (default: duckdb)
  persist_artifacts: true          # default: always write artifacts
  run_parity_validation: false     # local default; CI will set true
  memoization:
    enable: true
    cache_ttl_hours: 24
    min_cache_hit_percentage: 30  # Cache hit must show ≥30% reduction
  performance:
    target_seconds_94k: 50  # Target: <50s @94K
    benchmark_runs: 3  # Number of runs for median calculation

# Alias Matching Configuration
alias:
  optimize: true  # Enable optimized alias matching (Phase 1.21.1)
  workers: 12  # Number of parallel workers for alias matching
  progress_interval_s: 1.0  # Progress log interval in seconds
  page_size_options: [50, 100, 200, 500]
  max_page_fetch_seconds: 30  # Timeout threshold
  perf_feature_flags:
    paging_enabled: true
    emergency_small_pages: true
    hard_timeout_enabled: true
  duckdb_threads: 4
  max_pyarrow_group_stats_seconds: 5

# Schema Configuration (Phase 1.26.1 preparation)
schema:
  # Synonyms for column name mapping (Phase 1.26.1)
  synonyms:
    account_name: ["Account Name", "Company", "Company Name", "Customer", "Customer Name", "Name"]
    account_id: ["Account ID", "Acct ID", "Id", "ID", "External Id", "ExternalID"]
    parent_account_id: ["Parent Account ID", "Parent Id", "ParentID"]
    created_date: ["Created Date", "Create Date", "CreatedOn", "Created_On"]
    suffix_class: ["Suffix", "Company Suffix", "Entity Type"]
  
  # Templates for specific file patterns (Phase 1.26.1)
  templates:
    - match: "^company_junction_.*\\.csv$"
      aliases:
        account_name: ["Account Name"]
        account_id: ["Account ID"]
        created_date: ["Created Date"]

# Diagnostics Configuration
diagnostics:
  enable_block_stats: true
  output_dir: "data/interim"
  write_csv: true
  write_parquet: true
