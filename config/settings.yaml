# Company Junction Pipeline Configuration

# Data Processing Settings
data:
  # Default name column for duplicate detection
  name_column: "Account Name"
  
  # Supported file formats
  supported_formats: [".csv", ".xlsx", ".xls"]
  
  # Output file naming pattern
  output_pattern: "cleaned_{object_type}_{timestamp}.csv"

similarity:
  high: 92
  medium: 84
  penalty:
    suffix_mismatch: 25
    num_style_mismatch: 5
    punctuation_mismatch: 3
  max_alias_pairs: 100000
  performance:
    block_cap: 800  # Deterministic cap per block
    secondary_blocking: "first_two_tokens"  # or "char_bigrams"
    pair_threshold_medium: 0.63  # Keep existing threshold if stable
    enable_vectorized_prefilters: true  # Enable character/token length filters
    max_length_diff: 5  # Maximum character length difference for prefiltering

grouping:
  edge_gating:
    enabled: true
    require_high_to_primary: false
    allow_medium_plus_shared_token: true
    shared_token_source: "name_core_tokens"  # exclude stop tokens
    canopy_bound:
      enabled: true
      max_without_high_edge: 8

llm:
  enabled: false
  delete_threshold: 85

survivorship:
  tie_breakers: [created_date, account_id]

io:
  interim_format: parquet

# UI Performance Configuration
ui_perf:
  groups:
    use_stats_parquet: true         # rollback toggle
    duckdb_prefer_over_pyarrow: true
    rows_duckdb_threshold: 30000
    groups_duckdb_threshold: 10000
  details:
    use_details_parquet: true       # rollback toggle
    allow_pyarrow_fallback: false   # strict for MVP
    lru_capacity: 16
    auto_load_on_expand: true       # auto-load details when expander opens
    show_load_button: false         # hide button when auto-load enabled

# UI Configuration
ui:
  sort:
    default: "group_size DESC"      # Default sort when unknown sort key provided
  use_duckdb_for_groups: false     # Legacy flag for DuckDB preference

# Salesforce Integration
salesforce:
  # Default object types
  object_types: ["Account", "Contact", "Lead", "Opportunity"]
  
  # Batch size for API operations
  batch_size: 200
  
  # Retry settings for failed operations
  max_retries: 3
  retry_delay: 5

# Parallelism Configuration
parallelism:
  # Default number of workers (None for auto-detection)
  workers: null
  
  # Backend to use ('loky' for processes, 'threading' for threads)
  backend: "loky"
  
  # Chunk size for parallel processing
  chunk_size: 1000
  
  # Threshold for auto-switching to sequential execution
  small_input_threshold: 10000
  
  # Memory cap percentage (0.75 = 75% of total RAM)
  memory_cap_percent: 75.0

# Logging Configuration
logging:
  level: "INFO"

# Cleanup Configuration
cleanup:
  protect_latest: true
  pinned_runs: []
  max_age_days: null  # no default age limit
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "pipeline.log"

# CSV Processing Configuration
csv:
  # Engine preference for CSV reading (auto, pyarrow, c, python)
  engine: "auto"
  
  # Number of rows to sample for schema inference
  sample_rows: 10000
  
  # Columns to force to string type
  force_string_cols:
    - "account_id"
    - "parent_account_id"

# File Paths
paths:
  raw_data: "data/raw"
  interim_data: "data/interim"
  processed_data: "data/processed"
  test_fixtures: "tests/fixtures"

# UI Configuration
ui:
  page_size_default: 50  # Emergency reduction from 500
  timeout_seconds: 30  # Timeout for UI operations
  cache_capacity: 16  # LRU cache capacity for details
  duckdb_threads: 4  # Number of DuckDB threads
  max_pyarrow_group_stats_seconds: 5  # Auto-switch threshold for group stats
  use_duckdb_for_groups: true  # Force DuckDB routing for groups list

# Alias Matching Configuration
alias:
  optimize: true  # Enable optimized alias matching (Phase 1.21.1)
  progress_interval_s: 1.0  # Progress log interval in seconds
  page_size_options: [50, 100, 200, 500]
  max_page_fetch_seconds: 30  # Timeout threshold
  perf_feature_flags:
    paging_enabled: true
    emergency_small_pages: true
    hard_timeout_enabled: true
  duckdb_threads: 4
  max_pyarrow_group_stats_seconds: 5

# Schema Configuration (Phase 1.26.1 preparation)
schema:
  # Synonyms for column name mapping (Phase 1.26.1)
  synonyms:
    account_name: ["Account Name", "Company", "Company Name", "Customer", "Customer Name", "Name"]
    account_id: ["Account ID", "Acct ID", "Id", "ID", "External Id", "ExternalID"]
    parent_account_id: ["Parent Account ID", "Parent Id", "ParentID"]
    created_date: ["Created Date", "Create Date", "CreatedOn", "Created_On"]
    suffix_class: ["Suffix", "Company Suffix", "Entity Type"]
  
  # Templates for specific file patterns (Phase 1.26.1)
  templates:
    - match: "^company_junction_.*\\.csv$"
      aliases:
        account_name: ["Account Name"]
        account_id: ["Account ID"]
        created_date: ["Created Date"]
